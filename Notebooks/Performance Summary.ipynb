{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Comparative Performance Metrics\n",
    "\n",
    "To evaluate how closely the model replicates the legacy reimbursement system, we compared three evaluation metrics that capture different aspects of performance:\n",
    "\n",
    "| Metric | What It Measures | Why It Matters in This Project | Result | Interpretation |\n",
    "|---|---|---|---|---|\n",
    "| **MAE (Mean Absolute Error)** | Average difference (in dollars or cents) between the model and legacy output | Shows typical deviation; lower MAE means closer day-to-day matching | *X.XX cents* | Small MAE indicates the model captures core reimbursement patterns well. |\n",
    "| **RMSE (Root Mean Squared Error)** | Penalizes **larger errors** more heavily | Reveals if a few cases are significantly off due to threshold or rounding effects | *X.XX cents* | A low RMSE suggests no large outlier mismatches; a high RMSE suggests missing rules/quirks. |\n",
    "| **Accuracy within ¬± $1** | % of cases where the predicted reimbursement is **within $1.00** of the legacy output | Measures practical similarity even if not exact | *XX.X%* | High accuracy here means the model approximates behavior well, even if exact match logic needs refinement. |\n",
    "\n",
    "### Interpretation Guidelines\n",
    "\n",
    "- **If MAE is low but RMSE is high:**\n",
    "  ‚Üí The model works on most cases, but is failing in certain **threshold regions**.\n",
    "\n",
    "- **If both MAE and RMSE are low but exact-match rate is below target:**\n",
    "  ‚Üí The model‚Äôs **math logic is correct** but **rounding / step / quirk rules** need refinement.\n",
    "\n",
    "- **If Accuracy within ¬±$1 is high but EMR is low:**\n",
    "  ‚Üí The model is *behaviorally close* but not yet **bitwise replicating** the legacy engine.\n",
    "  This is expected **before** quirk-capture rules are added.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "ea3424f316889187"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ==========================================================\n",
    "# üöÄ Phase 2 ‚Äì Feature Engineering & Baseline Models (Refactored)\n",
    "# ==========================================================\n",
    "# Author: Ayushi Bohra (refactor with reusable evaluation functions by Matthew Fernald)\n",
    "# Project: ACME Corp ‚Äì Legacy Reimbursement System\n",
    "# ==========================================================\n",
    "\n",
    "# Step 0: Import Libraries\n",
    "# ----------------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from math import sqrt\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"crest\", font_scale=1.1)\n",
    "\n",
    "# ==========================================================\n",
    "# Utility Functions ‚Äì Data & Features\n",
    "# ==========================================================\n",
    "\n",
    "def load_clean_data(path=\"../data/combined_clean.csv\"):\n",
    "    \"\"\"Load cleaned combined dataset.\"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    print(\" Clean dataset loaded successfully!\")\n",
    "    print(\"Shape:\", df.shape)\n",
    "    display(df.head())\n",
    "    print(\"\\n Missing values per column:\")\n",
    "    print(df.isnull().sum())\n",
    "    return df\n",
    "\n",
    "\n",
    "def engineer_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Feature Engineering (Checklist: Data Integrity + Feature Behavior)\n",
    "    - cost_per_day\n",
    "    - cost_per_mile\n",
    "    - miles_per_day\n",
    "    - cost_ratio\n",
    "    Cleans NaNs and infinities.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"cost_per_day\"] = df[\"total_receipts_amount\"] / df[\"trip_duration_days\"].replace(0, np.nan)\n",
    "    df[\"cost_per_mile\"] = df[\"total_receipts_amount\"] / df[\"miles_traveled\"].replace(0, np.nan)\n",
    "    df[\"miles_per_day\"] = df[\"miles_traveled\"] / df[\"trip_duration_days\"].replace(0, np.nan)\n",
    "    df[\"cost_ratio\"] = df[\"cost_per_day\"] / df[\"cost_per_mile\"]\n",
    "\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    print(\"\\n Derived features created and cleaned successfully!\")\n",
    "    display(df[[\"cost_per_day\", \"cost_per_mile\", \"miles_per_day\", \"cost_ratio\"]].describe())\n",
    "    return df\n",
    "\n",
    "\n",
    "def detect_outliers_iqr(df: pd.DataFrame, columns):\n",
    "    \"\"\"\n",
    "    IQR-based outlier detection.\n",
    "    (Checklist: Data Integrity sanity check)\n",
    "    \"\"\"\n",
    "    print(\"\\nüìè Applying IQR Outlier Detection...\")\n",
    "    outlier_info = {}\n",
    "    for col in columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower = Q1 - 1.5 * IQR\n",
    "        upper = Q3 + 1.5 * IQR\n",
    "        outliers = df[(df[col] < lower) | (df[col] > upper)]\n",
    "        outlier_info[col] = outliers.shape[0]\n",
    "        print(f\"Outliers detected in {col}: {outliers.shape[0]}\")\n",
    "    print(\"\\n Outlier check complete.\")\n",
    "    print(\"Summary (count of detected outliers per feature):\")\n",
    "    print(outlier_info)\n",
    "    return outlier_info\n",
    "\n",
    "\n",
    "def plot_feature_distributions(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Simple distribution plots for engineered features.\n",
    "    (Helps with Feature Behavior & Data Integrity understanding)\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    sns.histplot(df[\"cost_per_day\"], kde=True, ax=axes[0, 0], color=\"olive\")\n",
    "    sns.histplot(df[\"cost_per_mile\"], kde=True, ax=axes[0, 1], color=\"seagreen\")\n",
    "    sns.histplot(df[\"miles_per_day\"], kde=True, ax=axes[1, 0], color=\"teal\")\n",
    "    sns.histplot(df[\"cost_ratio\"], kde=True, ax=axes[1, 1], color=\"darkcyan\")\n",
    "    axes[0, 0].set_title(\"Cost per Day\")\n",
    "    axes[0, 1].set_title(\"Cost per Mile\")\n",
    "    axes[1, 0].set_title(\"Miles per Day\")\n",
    "    axes[1, 1].set_title(\"Cost Ratio\")\n",
    "    plt.suptitle(\" Distribution of Derived Features\", fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def prepare_train_test(df: pd.DataFrame, feature_cols, target_col, train_frac=0.75):\n",
    "    \"\"\"\n",
    "    NOTE: uses first 75% of rows as train, last 25% as test.\n",
    "    Manual 75/25 train‚Äìtest split (keeps behavior of original notebook).\n",
    "    Returns X_train, X_test, y_train, y_test, split_index.\n",
    "    \"\"\"\n",
    "    X = df[feature_cols]\n",
    "    y = df[target_col]\n",
    "\n",
    "    split = int(train_frac * len(df))\n",
    "    X_train, X_test = X[:split], X[split:]\n",
    "    y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "    print(f\"\\n Training samples: {len(X_train)}, Testing samples: {len(X_test)}\")\n",
    "    return X_train, X_test, y_train, y_test, split\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# Core Evaluation Helpers ‚Äì Checklist Metrics\n",
    "# ==========================================================\n",
    "\n",
    "def evaluate_model_predictions(y_true, y_pred, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Success Criteria (with counts and percentages):\n",
    "      - Exact matches: abs diff <= $0.01\n",
    "      - Close matches: abs diff <= $1.00\n",
    "      - Within $5 matches: abs diff <= $5.00\n",
    "      - Score: MAE + RMSE (lower is better)\n",
    "      - Additional metrics: R¬≤\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    abs_diff = np.abs(y_pred - y_true)\n",
    "    n = len(y_true)\n",
    "\n",
    "    # Counts\n",
    "    exact_count = (abs_diff <= 0.01).sum()\n",
    "    close_count = (abs_diff <= 1.00).sum()\n",
    "    within5_count = (abs_diff <= 5.00).sum()\n",
    "\n",
    "    # Rates\n",
    "    exact_rate = exact_count / n\n",
    "    close_rate = close_count / n\n",
    "    within5_rate = within5_count / n\n",
    "\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    score = mae + rmse\n",
    "\n",
    "    return {\n",
    "        \"Model\": model_name,\n",
    "        \"N\": n,\n",
    "        # Counts\n",
    "        \"Exact Matches (count)\": exact_count,\n",
    "        \"Close Matches (count)\": close_count,\n",
    "        \"¬±$5 Matches (count)\": within5_count,\n",
    "        # Rates (used by gate + slices)\n",
    "        \"Exact Match Rate (<= $0.01)\": exact_rate,\n",
    "        \"Close Match Rate (<= $1.00)\": close_rate,\n",
    "        \"¬±$5 Accuracy\": within5_rate,\n",
    "        # Error metrics\n",
    "        \"MAE\": mae,\n",
    "        \"RMSE\": rmse,\n",
    "        \"R¬≤\": r2,\n",
    "        \"Score (lower=better)\": score\n",
    "    }\n",
    "\n",
    "\n",
    "def _print_eval_block(eval_dict):\n",
    "    \"\"\"\n",
    "    Nicely formatted print for train/test blocks using counts + rates.\n",
    "    \"\"\"\n",
    "    n = eval_dict[\"N\"]\n",
    "    print(f\"Model: {eval_dict['Model']}\")\n",
    "    print(\n",
    "        f\"Exact Matches: {eval_dict['Exact Matches (count)']}/{n} \"\n",
    "        f\"({eval_dict['Exact Match Rate (<= $0.01)']:.3f})\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Close Matches: {eval_dict['Close Matches (count)']}/{n} \"\n",
    "        f\"({eval_dict['Close Match Rate (<= $1.00)']:.3f})\"\n",
    "    )\n",
    "    print(\n",
    "        f\"$5 Accuracy: {eval_dict['¬±$5 Matches (count)']}/{n} \"\n",
    "        f\"({eval_dict['¬±$5 Accuracy']:.3f})\"\n",
    "    )\n",
    "    print(f\"MAE: {eval_dict['MAE']:.3f}\")\n",
    "    print(f\"RMSE: {eval_dict['RMSE']:.3f}\")\n",
    "    print(f\"R¬≤: {eval_dict['R¬≤']:.3f}\")\n",
    "    print(f\"Score (lower=better): {eval_dict['Score (lower=better)']:.3f}\")\n",
    "\n",
    "\n",
    "def compare_train_test_generalization(model, X_train, y_train, X_test, y_test, model_name):\n",
    "    \"\"\"\n",
    "    Fit model (if not already fitted) and compare train vs test.\n",
    "    (Checklist: Generalization)\n",
    "    \"\"\"\n",
    "    # Fit model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    train_pred = model.predict(X_train)\n",
    "    test_pred = model.predict(X_test)\n",
    "\n",
    "    train_eval = evaluate_model_predictions(y_train, train_pred, f\"{model_name} ‚Äì Train\")\n",
    "    test_eval = evaluate_model_predictions(y_test, test_pred, f\"{model_name} ‚Äì Test\")\n",
    "\n",
    "    print(\"=====  Train vs Test Comparison =====\")\n",
    "\n",
    "    print(\"\\nTrain set:\")\n",
    "    _print_eval_block(train_eval)\n",
    "\n",
    "    print(\"\\nTest set:\")\n",
    "    _print_eval_block(test_eval)\n",
    "\n",
    "    print(\"\\nInterpretation:\")\n",
    "    print(\"- If Train R¬≤ >> Test R¬≤ and Train MAE << Test MAE ‚Üí likely overfitting.\")\n",
    "    print(\"- If they are similar ‚Üí generalization is reasonable for the current feature set.\")\n",
    "\n",
    "    return train_eval, test_eval, test_pred\n",
    "\n",
    "\n",
    "def slice_analysis_by_duration(df_with_preds, y_true_col=\"y_true\", y_pred_col=\"y_pred\"):\n",
    "    \"\"\"\n",
    "    Slice analysis WITHOUT binning.\n",
    "    Original-style global evaluation over the entire test set.\n",
    "    (Checklist: Slice Analysis ‚Äì here used as a single overall slice.)\n",
    "    \"\"\"\n",
    "    stats = evaluate_model_predictions(\n",
    "        df_with_preds[y_true_col],\n",
    "        df_with_preds[y_pred_col],\n",
    "        model_name=\"All Durations\"\n",
    "    )\n",
    "\n",
    "    # Build a one-row DataFrame that looks like a \"slice table\"\n",
    "    slice_df = pd.DataFrame([{\n",
    "        \"Slice\": \"All Durations\",\n",
    "        \"N\": stats[\"N\"],\n",
    "        \"Exact Matches (count)\": stats[\"Exact Matches (count)\"],\n",
    "        \"Close Matches (count)\": stats[\"Close Matches (count)\"],\n",
    "        \"¬±$5 Matches (count)\": stats[\"¬±$5 Matches (count)\"],\n",
    "        \"Exact Match Rate (<= $0.01)\": stats[\"Exact Match Rate (<= $0.01)\"],\n",
    "        \"Close Match Rate (<= $1.00)\": stats[\"Close Match Rate (<= $1.00)\"],\n",
    "        \"¬±$5 Accuracy\": stats[\"¬±$5 Accuracy\"],\n",
    "        \"MAE\": stats[\"MAE\"],\n",
    "        \"RMSE\": stats[\"RMSE\"],\n",
    "        \"R¬≤\": stats[\"R¬≤\"],\n",
    "    }])\n",
    "\n",
    "    print(\"===== üîç Slice Analysis (no binning ‚Äì full test set) =====\")\n",
    "    display(\n",
    "        slice_df.style.format(\n",
    "            {\n",
    "                \"Exact Match Rate (<= $0.01)\": \"{:.3f}\",\n",
    "                \"Close Match Rate (<= $1.00)\": \"{:.3f}\",\n",
    "                \"¬±$5 Accuracy\": \"{:.3f}\",\n",
    "                \"MAE\": \"{:.3f}\",\n",
    "                \"RMSE\": \"{:.3f}\",\n",
    "                \"R¬≤\": \"{:.3f}\",\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return slice_df\n",
    "\n",
    "\n",
    "\n",
    "def rounding_and_quirks_analysis(df_with_preds, y_true_col=\"y_true\", y_pred_col=\"y_pred\"):\n",
    "    \"\"\"\n",
    "    Rounding & quirks analysis based on cent distributions.\n",
    "    (Checklist: Rounding & Quirks)\n",
    "    \"\"\"\n",
    "    true_cents = (np.round(df_with_preds[y_true_col], 2) * 100).astype(int) % 100\n",
    "    pred_cents = (np.round(df_with_preds[y_pred_col], 2) * 100).astype(int) % 100\n",
    "\n",
    "    true_counts = true_cents.value_counts(normalize=True).sort_index()\n",
    "    pred_counts = pred_cents.value_counts(normalize=True).sort_index()\n",
    "\n",
    "    rounding_compare = pd.DataFrame({\n",
    "        \"True Frequency\": true_counts,\n",
    "        \"Pred Frequency\": pred_counts\n",
    "    }).fillna(0)\n",
    "\n",
    "    print(\"Top 10 cent values by true frequency (Test Set):\")\n",
    "    display(rounding_compare.sort_values(\"True Frequency\", ascending=False).head(10))\n",
    "    print(\"\\nUse this to see if the model reproduces quirks like common .00, .25, .50 patterns.\")\n",
    "\n",
    "    return rounding_compare\n",
    "\n",
    "\n",
    "def explainability_linear_model(X, y):\n",
    "    \"\"\"\n",
    "    Train a simple global linear model and show coefficients.\n",
    "    (Checklist: Explainability ‚Äì feature influence overview)\n",
    "    \"\"\"\n",
    "    lin_all = LinearRegression()\n",
    "    lin_all.fit(X, y)\n",
    "\n",
    "    coef_df = pd.DataFrame({\n",
    "        \"feature\": X.columns,\n",
    "        \"coefficient\": lin_all.coef_\n",
    "    }).sort_values(\"coefficient\", key=lambda s: s.abs(), ascending=False)\n",
    "\n",
    "    print(\"Linear model coefficients (sorted by |coefficient|):\")\n",
    "    display(coef_df)\n",
    "    return coef_df\n",
    "\n",
    "\n",
    "def final_success_gate(eval_dict,\n",
    "                       MIN_R2=0.85,\n",
    "                       MAX_MAE=200.0,\n",
    "                       MIN_CLOSE_MATCH_RATE=0.30,       # within $1\n",
    "                       MIN_WITHIN_5_DOLLAR_RATE=0.80):  # within $5\n",
    "    \"\"\"\n",
    "    Final success gate check against thresholds.\n",
    "    (Checklist: Final Success Gate)\n",
    "    Uses tolerance accuracy metrics to determine readiness.\n",
    "    \"\"\"\n",
    "    meets_r2 = eval_dict[\"R¬≤\"] >= MIN_R2\n",
    "    meets_mae = eval_dict[\"MAE\"] <= MAX_MAE\n",
    "    meets_close = eval_dict[\"Close Match Rate (<= $1.00)\"] >= MIN_CLOSE_MATCH_RATE\n",
    "    meets_within5 = eval_dict[\"¬±$5 Accuracy\"] >= MIN_WITHIN_5_DOLLAR_RATE\n",
    "\n",
    "    all_pass = meets_r2 and meets_mae and meets_close and meets_within5\n",
    "\n",
    "    print(\"=====  Final Success Gate (Test Set) =====\")\n",
    "\n",
    "    print(f\"Exact Matches (count):                {eval_dict['Exact Matches (count)']}/{eval_dict['N']}\")\n",
    "    print(f\"Close Matches (<= $1):               {eval_dict['Close Matches (count)']}/{eval_dict['N']} ({eval_dict['Close Match Rate (<= $1.00)']:.3f})\")\n",
    "    print(f\"¬±$5 Matches:                         {eval_dict['¬±$5 Matches (count)']}/{eval_dict['N']} ({eval_dict['¬±$5 Accuracy']:.3f})\")\n",
    "\n",
    "    print(f\"R¬≤ >= {MIN_R2}:                      {meets_r2} (actual: {eval_dict['R¬≤']:.3f})\")\n",
    "    print(f\"MAE <= {MAX_MAE}:                    {meets_mae} (actual: {eval_dict['MAE']:.3f})\")\n",
    "    print(f\"Close Match Rate >= {MIN_CLOSE_MATCH_RATE}: {meets_close} (actual: {eval_dict['Close Match Rate (<= $1.00)']:.3f})\")\n",
    "    print(f\"¬±$5 Accuracy >= {MIN_WITHIN_5_DOLLAR_RATE}: {meets_within5} (actual: {eval_dict['¬±$5 Accuracy']:.3f})\")\n",
    "\n",
    "\n",
    "    if all_pass:\n",
    "        print(\"\\n Model PASSES Final Success Gate ‚Äì OK to move to next phase.\")\n",
    "    else:\n",
    "        print(\"\\n Model DOES NOT meet all Final Success Gate criteria ‚Äì needs refinement.\")\n",
    "\n",
    "    return all_pass\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# High-Level Runner ‚Äì ‚ÄúTest this model with the checklist‚Äù\n",
    "# ==========================================================\n",
    "\n",
    "def run_full_checklist_for_model(model, model_name, df, feature_cols, target_col, train_frac=0.75):\n",
    "    \"\"\"\n",
    "    High-level function that:\n",
    "      1) Prepares train/test\n",
    "      2) Fits model & compares train vs test\n",
    "      3) Builds test_df with predictions\n",
    "      4) Runs slice analysis\n",
    "      5) Runs rounding/quirks analysis\n",
    "      6) Runs explainability (linear surrogate)\n",
    "      7) Applies final success gate\n",
    "    \"\"\"\n",
    "    # 1. Train‚Äìtest\n",
    "    X_train, X_test, y_train, y_test, split_idx = prepare_train_test(\n",
    "        df, feature_cols, target_col, train_frac=train_frac\n",
    "    )\n",
    "\n",
    "    # 2. Generalization + core metrics\n",
    "    train_eval, test_eval, test_pred = compare_train_test_generalization(\n",
    "        model, X_train, y_train, X_test, y_test, model_name=model_name\n",
    "    )\n",
    "\n",
    "    # 3. Build test-only DataFrame with preds for slice & quirks\n",
    "    test_df = df.iloc[split_idx:].copy().reset_index(drop=True)\n",
    "    test_df[\"y_true\"] = y_test.values\n",
    "    test_df[\"y_pred\"] = test_pred\n",
    "\n",
    "    # 4. Slice analysis\n",
    "    slice_df = slice_analysis_by_duration(test_df, y_true_col=\"y_true\", y_pred_col=\"y_pred\")\n",
    "\n",
    "    # 5. Rounding & quirks\n",
    "    rounding_df = rounding_and_quirks_analysis(test_df, y_true_col=\"y_true\", y_pred_col=\"y_pred\")\n",
    "\n",
    "    # 6. Explainability ‚Äì global linear on full data\n",
    "    X_full = df[feature_cols]\n",
    "    y_full = df[target_col]\n",
    "    coef_df = explainability_linear_model(X_full, y_full)\n",
    "\n",
    "    # 7. Final Success Gate using test_eval (checklist verdict)\n",
    "    gate_pass = final_success_gate(test_eval)\n",
    "\n",
    "    return {\n",
    "        \"train_eval\": train_eval,\n",
    "        \"test_eval\": test_eval,\n",
    "        \"slice_df\": slice_df,\n",
    "        \"rounding_df\": rounding_df,\n",
    "        \"coef_df\": coef_df,\n",
    "        \"success_gate_passed\": gate_pass\n",
    "    }\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "#  MAIN: Run Everything for Polynomial (deg=2) Model\n",
    "# ==========================================================\n",
    "\n",
    "# 1. Load & engineer\n",
    "combined_df = load_clean_data(\"../data/combined_clean.csv\")\n",
    "combined_df = engineer_features(combined_df)\n",
    "\n",
    "# 2. Optional: outlier detection + plots\n",
    "numeric_cols = [\n",
    "    \"trip_duration_days\", \"miles_traveled\", \"total_receipts_amount\",\n",
    "    \"reimbursement\", \"cost_per_day\", \"cost_per_mile\", \"miles_per_day\", \"cost_ratio\"\n",
    "]\n",
    "_ = detect_outliers_iqr(combined_df, numeric_cols)\n",
    "plot_feature_distributions(combined_df)\n",
    "\n",
    "# 3. Define features & target\n",
    "features = [\n",
    "    \"trip_duration_days\",\n",
    "    \"miles_traveled\",\n",
    "    \"total_receipts_amount\",\n",
    "    \"cost_per_day\",\n",
    "    \"cost_per_mile\",\n",
    "    \"miles_per_day\",\n",
    "    \"cost_ratio\"\n",
    "]\n",
    "target = \"reimbursement\"\n",
    "\n",
    "# 4. Define the model you want to test (here: Polynomial deg=2)\n",
    "poly_model = make_pipeline(\n",
    "    PolynomialFeatures(degree=2, include_bias=False),\n",
    "    LinearRegression()\n",
    ")\n",
    "\n",
    "# 5. Run full checklist evaluation for this model\n",
    "results = run_full_checklist_for_model(\n",
    "    model=poly_model,\n",
    "    model_name=\"Polynomial (deg=2)\",\n",
    "    df=combined_df,\n",
    "    feature_cols=features,\n",
    "    target_col=target,\n",
    "    train_frac=0.75\n",
    ")\n",
    "\n",
    "# `results` now holds all the structured outputs you might want to use in your report.\n"
   ],
   "id": "ca6f904639f72c22"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
