{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ==========================================================\n",
    "# Phase 3 – Ensemble Learning (Stacking Regressor)\n",
    "# ==========================================================\n",
    "\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from math import sqrt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# ---------------------------------------\n",
    "# Rule-based feature engineering\n",
    "# ---------------------------------------\n",
    "\n",
    "def add_rule_based_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add rule-based / threshold features on top of Phase 2 engineered features.\n",
    "\n",
    "    Assumes df has:\n",
    "      - trip_duration_days\n",
    "      - miles_traveled\n",
    "      - total_receipts_amount\n",
    "      - cost_per_day\n",
    "      - cost_per_mile\n",
    "      - miles_per_day\n",
    "      - cost_ratio\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # --- Trip duration buckets ---\n",
    "    df[\"is_short_trip\"]  = (df[\"trip_duration_days\"] <= 3).astype(int)\n",
    "    df[\"is_medium_trip\"] = df[\"trip_duration_days\"].between(4, 7).astype(int)\n",
    "    df[\"is_long_trip\"]   = (df[\"trip_duration_days\"] >= 8).astype(int)\n",
    "\n",
    "    # --- Mileage buckets ---\n",
    "    df[\"is_local_miles\"]    = (df[\"miles_traveled\"] <= 50).astype(int)\n",
    "    df[\"is_regional_miles\"] = df[\"miles_traveled\"].between(51, 300).astype(int)\n",
    "    df[\"is_long_miles\"]     = df[\"miles_traveled\"].between(301, 800).astype(int)\n",
    "    df[\"is_very_long_miles\"]= (df[\"miles_traveled\"] > 800).astype(int)\n",
    "\n",
    "    # --- Receipt amount buckets ---\n",
    "    df[\"receipts_low\"]    = (df[\"total_receipts_amount\"] <= 100).astype(int)\n",
    "    df[\"receipts_mid\"]    = df[\"total_receipts_amount\"].between(100, 500).astype(int)\n",
    "    df[\"receipts_high\"]   = df[\"total_receipts_amount\"].between(500, 1500).astype(int)\n",
    "    df[\"receipts_extreme\"]= (df[\"total_receipts_amount\"] > 1500).astype(int)\n",
    "\n",
    "    # --- Cost-per-mile thresholds ---\n",
    "    df[\"cpm_very_low\"] = (df[\"cost_per_mile\"] < 0.5).astype(int)\n",
    "    df[\"cpm_low\"]      = df[\"cost_per_mile\"].between(0.5, 1.5).astype(int)\n",
    "    df[\"cpm_mid\"]      = df[\"cost_per_mile\"].between(1.5, 5.0).astype(int)\n",
    "    df[\"cpm_high\"]     = (df[\"cost_per_mile\"] > 5.0).astype(int)\n",
    "\n",
    "    # --- Cost-per-day thresholds ---\n",
    "    df[\"cpd_low\"]    = (df[\"cost_per_day\"] <= 150).astype(int)\n",
    "    df[\"cpd_mid\"]    = df[\"cost_per_day\"].between(150, 300).astype(int)\n",
    "    df[\"cpd_high\"]   = df[\"cost_per_day\"].between(300, 600).astype(int)\n",
    "    df[\"cpd_extreme\"]= (df[\"cost_per_day\"] > 600).astype(int)\n",
    "\n",
    "    # --- Miles per day: \"mileage-heavy\" trips ---\n",
    "    df[\"miles_per_day_light\"]  = (df[\"miles_per_day\"] <= 50).astype(int)\n",
    "    df[\"miles_per_day_normal\"] = df[\"miles_per_day\"].between(51, 200).astype(int)\n",
    "    df[\"miles_per_day_heavy\"]  = (df[\"miles_per_day\"] > 200).astype(int)\n",
    "\n",
    "    # --- Simple interaction-style flags (candidate rules) ---\n",
    "    # Short trips with high receipts → maybe special handling\n",
    "    df[\"short_high_receipts\"] = ((df[\"trip_duration_days\"] <= 3) &\n",
    "                                 (df[\"total_receipts_amount\"] > 500)).astype(int)\n",
    "\n",
    "    # Long trips with low miles → possibly hotel-heavy, mileage-light\n",
    "    df[\"long_low_miles\"] = ((df[\"trip_duration_days\"] >= 8) &\n",
    "                            (df[\"miles_traveled\"] < 100)).astype(int)\n",
    "\n",
    "    # High mileage & low receipts → maybe mileage-structured\n",
    "    df[\"high_miles_low_receipts\"] = ((df[\"miles_traveled\"] > 500) &\n",
    "                                     (df[\"total_receipts_amount\"] < 300)).astype(int)\n",
    "\n",
    "    # Ratio-based: receipts per mile/day above a rough threshold\n",
    "    df[\"receipt_heavy_per_mile\"] = ((df[\"total_receipts_amount\"] /\n",
    "                                     df[\"miles_traveled\"].replace(0, np.nan)) > 3).fillna(0).astype(int)\n",
    "    df[\"receipt_heavy_per_day\"]  = ((df[\"total_receipts_amount\"] /\n",
    "                                     df[\"trip_duration_days\"].replace(0, np.nan)) > 300).fillna(0).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Load the enhanced dataset from Phase 2\n",
    "combined_df = pd.read_csv(\"phase2_features_baseline_models.csv\")\n",
    "\n",
    "print(\"Dataset loaded for Phase 3!\")\n",
    "print(\"Shape:\", combined_df.shape)\n",
    "\n",
    "# Add rule-based features on top of Phase 2 engineered features\n",
    "combined_df = add_rule_based_features(combined_df)\n",
    "print(\"Rule-based features added. New shape:\", combined_df.shape)\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "# Select Features and Target\n",
    "# ---------------------------------------\n",
    "features = [\n",
    "    # original continuous features\n",
    "    \"trip_duration_days\",\n",
    "    \"miles_traveled\",\n",
    "    \"total_receipts_amount\",\n",
    "    \"cost_per_day\",\n",
    "    \"cost_per_mile\",\n",
    "    \"miles_per_day\",\n",
    "    \"cost_ratio\",\n",
    "\n",
    "    # rule-based trip duration flags\n",
    "    \"is_short_trip\",\n",
    "    \"is_medium_trip\",\n",
    "    \"is_long_trip\",\n",
    "\n",
    "    # rule-based mileage flags\n",
    "    \"is_local_miles\",\n",
    "    \"is_regional_miles\",\n",
    "    \"is_long_miles\",\n",
    "    \"is_very_long_miles\",\n",
    "\n",
    "    # receipts buckets\n",
    "    \"receipts_low\",\n",
    "    \"receipts_mid\",\n",
    "    \"receipts_high\",\n",
    "    \"receipts_extreme\",\n",
    "\n",
    "    # cost per mile buckets\n",
    "    \"cpm_very_low\",\n",
    "    \"cpm_low\",\n",
    "    \"cpm_mid\",\n",
    "    \"cpm_high\",\n",
    "\n",
    "    # cost per day buckets\n",
    "    \"cpd_low\",\n",
    "    \"cpd_mid\",\n",
    "    \"cpd_high\",\n",
    "    \"cpd_extreme\",\n",
    "\n",
    "    # miles per day buckets\n",
    "    \"miles_per_day_light\",\n",
    "    \"miles_per_day_normal\",\n",
    "    \"miles_per_day_heavy\",\n",
    "\n",
    "    # interaction-style flags\n",
    "    \"short_high_receipts\",\n",
    "    \"long_low_miles\",\n",
    "    \"high_miles_low_receipts\",\n",
    "    \"receipt_heavy_per_mile\",\n",
    "    \"receipt_heavy_per_day\",\n",
    "]\n",
    "\n",
    "\n",
    "target = \"reimbursement\"\n",
    "\n",
    "X = combined_df[features]\n",
    "y = combined_df[target]\n",
    "\n",
    "# ---------------------------------------\n",
    "# Manual 75/25 split\n",
    "# ---------------------------------------\n",
    "split = int(0.75 * len(combined_df))\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "print(f\"Training Samples: {len(X_train)}\")\n",
    "print(f\"Testing Samples:  {len(X_test)}\")\n",
    "print(\" Training Ensemble Model...\")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Define Base Models\n",
    "# ----------------------------------------------------------\n",
    "base_models = [\n",
    "    ('decision_tree', DecisionTreeRegressor(random_state=42)),\n",
    "    ('random_forest', RandomForestRegressor(n_estimators=200, random_state=42)),\n",
    "    ('gradient_boosting', GradientBoostingRegressor(random_state=42))\n",
    "]\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Meta-Learner (Final model that blends predictions)\n",
    "# ----------------------------------------------------------\n",
    "meta_model = LinearRegression()\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Build Stacking Regressor\n",
    "# ----------------------------------------------------------\n",
    "stack_model = StackingRegressor(\n",
    "    estimators=base_models,\n",
    "    final_estimator=meta_model,\n",
    "    passthrough=True  # includes original features + model predictions\n",
    ")\n",
    "\n",
    "# Train Stacking Ensemble\n",
    "stack_model.fit(X_train, y_train)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Evaluate Model\n",
    "# ----------------------------------------------------------\n",
    "stack_pred = stack_model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, stack_pred)\n",
    "rmse = sqrt(mean_squared_error(y_test, stack_pred))\n",
    "r2 = r2_score(y_test, stack_pred)\n",
    "\n",
    "print(\"\\n Ensemble Model Performance (Stacking Regressor):\")\n",
    "print(f\"MAE:  {mae:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"R²:   {r2:.4f}\")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Plot Actual vs Predicted\n",
    "# ----------------------------------------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.scatterplot(x=y_test, y=stack_pred, alpha=0.7, color='teal')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.xlabel(\"Actual Reimbursement\")\n",
    "plt.ylabel(\"Predicted Reimbursement (Ensemble)\")\n",
    "plt.title(\"Actual vs Predicted — Stacking Ensemble\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#testing the new model\n",
    "def prediction_match_report(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Inline prediction match test (no CSV needed).\n",
    "\n",
    "    - Exact matches: |pred - actual| <= $0.01\n",
    "    - Close matches: |pred - actual| <= $1.00\n",
    "    - ±$5 Accuracy:  |pred - actual| <= $5.00\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    abs_diff = np.abs(y_pred - y_true)\n",
    "\n",
    "    exact = (abs_diff <= 0.01).sum() / len(y_true)\n",
    "    close = (abs_diff <= 1.00).sum() / len(y_true)\n",
    "    within_5 = (abs_diff <= 5.00).sum() / len(y_true)\n",
    "\n",
    "    print(\"\\n=====  Phase 3 Prediction Match Test =====\")\n",
    "    print(f\"Exact Match Rate (<= $0.01): {exact:.4f}\")\n",
    "    print(f\"Close Match Rate (<= $1.00): {close:.4f}\")\n",
    "    print(f\"±$5 Accuracy: {within_5:.4f}\")\n",
    "\n",
    "# Inline prediction match evaluation (no CSV)\n",
    "prediction_match_report(y_test, stack_pred)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "#  Residual Calibration – Learn Corrections on Top of Ensemble\n",
    "# ----------------------------------------------------------\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# 1) Compute base predictions on TRAIN and TEST\n",
    "train_base_pred = stack_model.predict(X_train)\n",
    "test_base_pred = stack_pred  # already computed above on X_test\n",
    "\n",
    "# 2) Fit residual model on training residuals (actual - base_pred)\n",
    "train_residuals = y_train - train_base_pred\n",
    "\n",
    "residual_model = GradientBoostingRegressor(\n",
    "    random_state=42,\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.05,   # smaller steps\n",
    "    max_depth=2,          # shallow trees to avoid overfitting\n",
    "    min_samples_leaf=20   # smoother corrections\n",
    ")\n",
    "\n",
    "residual_model.fit(X_train, train_residuals)\n",
    "\n",
    "print(\"\\n Residual correction model trained!\")\n",
    "\n",
    "# 3) Get correction predictions for train and test\n",
    "train_corrections = residual_model.predict(X_train)\n",
    "test_corrections = residual_model.predict(X_test)\n",
    "\n",
    "# 4) Calibrated final predictions\n",
    "train_final_pred = train_base_pred + train_corrections\n",
    "test_final_pred = test_base_pred + test_corrections\n",
    "\n",
    "# 5) Evaluate calibrated model\n",
    "calib_mae = mean_absolute_error(y_test, test_final_pred)\n",
    "calib_rmse = sqrt(mean_squared_error(y_test, test_final_pred))\n",
    "calib_r2 = r2_score(y_test, test_final_pred)\n",
    "\n",
    "print(\"\\n  Calibrated Ensemble Performance (with Residual Model):\")\n",
    "print(f\"MAE:  {calib_mae:.4f}\")\n",
    "print(f\"RMSE: {calib_rmse:.4f}\")\n",
    "print(f\"R²:   {calib_r2:.4f}\")\n",
    "\n",
    "# 6) Match-rate diagnostics for base vs calibrated model\n",
    "print(\"\\n=== Base Ensemble Match Rates (Test) ===\")\n",
    "prediction_match_report(y_test, test_base_pred)\n",
    "\n",
    "print(\"\\n=== Calibrated Ensemble Match Rates (Test) ===\")\n",
    "prediction_match_report(y_test, test_final_pred)\n",
    "\n",
    "# 7) (Optional) Also see how calibration behaves on train set\n",
    "print(\"\\n=== Calibrated Ensemble Match Rates (Train) ===\")\n",
    "prediction_match_report(y_train, train_final_pred)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "9dff3a06c8047fc7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ==========================================================\n",
    "# Phase 3 – Ensemble Learning (Stacking Regressor)\n",
    "# With ONLY Original + PRD Logic Features (NO rule flags)\n",
    "# ==========================================================\n",
    "\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from math import sqrt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# ---------------------------------------\n",
    "# PRD-driven feature engineering\n",
    "# ---------------------------------------\n",
    "\n",
    "def add_prd_logic_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds PRD-derived behavioral features:\n",
    "      - spend_per_day curved behavior\n",
    "      - efficiency reward band (4–6 days & 180–220 mi/day)\n",
    "      - log miles decay\n",
    "      - receipt cents anomalies (.49 / .99)\n",
    "      - receipts curve near $700 (bonus)\n",
    "      - extreme receipt conditions\n",
    "    Assumes Phase 2 features already exist:\n",
    "        cost_per_day, miles_traveled, trip_duration_days, total_receipts_amount\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # === Core Derived Behavior ===\n",
    "    df[\"spend_per_day\"] = df[\"cost_per_day\"]\n",
    "\n",
    "    df[\"miles_per_day_safe\"] = (\n",
    "        df[\"miles_traveled\"] / df[\"trip_duration_days\"].clip(lower=1)\n",
    "    )\n",
    "\n",
    "    df[\"log_miles_traveled\"] = np.log1p(df[\"miles_traveled\"])\n",
    "\n",
    "    # === Receipt cents anomalies (.49 / .99 quirk) ===\n",
    "    df[\"receipt_cents\"] = np.round(\n",
    "        df[\"total_receipts_amount\"] - np.floor(df[\"total_receipts_amount\"]), 2\n",
    "    )\n",
    "    df[\"receipt_is_point_49_or_99\"] = df[\"receipt_cents\"].isin([0.49, 0.99]).astype(int)\n",
    "\n",
    "    # === Spend per day optimal band (75–120) & penalties ===\n",
    "    df[\"spend_per_day_good_band\"] = df[\"spend_per_day\"].between(75, 120).astype(int)\n",
    "    df[\"spend_per_day_low\"] = (df[\"spend_per_day\"] < 50).astype(int)\n",
    "    df[\"spend_per_day_high\"] = (df[\"spend_per_day\"] > 120).astype(int)\n",
    "\n",
    "    # === Receipt non-linearity around ~700 ===\n",
    "    df[\"receipts_near_700\"] = df[\"total_receipts_amount\"].between(600, 800).astype(int)\n",
    "    df[\"receipts_very_low\"] = (df[\"total_receipts_amount\"] < 50).astype(int)\n",
    "    df[\"receipts_very_high\"] = (df[\"total_receipts_amount\"] > 1000).astype(int)\n",
    "\n",
    "    # === Efficiency Sweet Spot (4–6 days ∧ 180–220 mi/day) ===\n",
    "    duration_mask = df[\"trip_duration_days\"].between(4, 6)\n",
    "    miles_mask = df[\"miles_per_day_safe\"].between(180, 220)\n",
    "\n",
    "    df[\"is_efficiency_sweet_spot\"] = (duration_mask & miles_mask).astype(int)\n",
    "    df[\"efficiency_score\"] = (\n",
    "        df[\"miles_per_day_safe\"] * df[\"is_efficiency_sweet_spot\"]\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Load Phase 2 dataset + add PRD features\n",
    "# ----------------------------------------------------------\n",
    "combined_df = pd.read_csv(\"phase2_features_baseline_models.csv\")\n",
    "\n",
    "print(\"Dataset loaded for Phase 3!\")\n",
    "print(\"Shape:\", combined_df.shape)\n",
    "\n",
    "combined_df = add_prd_logic_features(combined_df)\n",
    "print(\"PRD-driven logic features added. New shape:\", combined_df.shape)\n",
    "\n",
    "# ---------------------------------------\n",
    "# Feature Selection (ONLY original + PRD)\n",
    "# ---------------------------------------\n",
    "features = [\n",
    "    # Phase 2 continuous engineered inputs\n",
    "    \"trip_duration_days\",\n",
    "    \"miles_traveled\",\n",
    "    \"total_receipts_amount\",\n",
    "    \"cost_per_day\",\n",
    "    \"cost_per_mile\",\n",
    "    \"miles_per_day\",\n",
    "    \"cost_ratio\",\n",
    "\n",
    "    # PRD logic features\n",
    "    \"spend_per_day\",\n",
    "    \"miles_per_day_safe\",\n",
    "    \"log_miles_traveled\",\n",
    "    \"receipt_cents\",\n",
    "    \"receipt_is_point_49_or_99\",\n",
    "    \"spend_per_day_good_band\",\n",
    "    \"spend_per_day_low\",\n",
    "    \"spend_per_day_high\",\n",
    "    \"receipts_near_700\",\n",
    "    \"receipts_very_low\",\n",
    "    \"receipts_very_high\",\n",
    "    \"efficiency_score\",\n",
    "    \"is_efficiency_sweet_spot\",\n",
    "]\n",
    "\n",
    "target = \"reimbursement\"\n",
    "\n",
    "X = combined_df[features]\n",
    "y = combined_df[target]\n",
    "\n",
    "# ---------------------------------------\n",
    "# Manual 75/25 split\n",
    "# ---------------------------------------\n",
    "split = int(0.75 * len(combined_df))\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "print(f\"Training Samples: {len(X_train)}\")\n",
    "print(f\"Testing Samples:  {len(X_test)}\")\n",
    "print(\" Training Ensemble Model...\")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Define Base & Stacking (same)\n",
    "# ----------------------------------------------------------\n",
    "base_models = [\n",
    "    ('decision_tree', DecisionTreeRegressor(random_state=42)),\n",
    "    ('random_forest', RandomForestRegressor(n_estimators=200, random_state=42)),\n",
    "    ('gradient_boosting', GradientBoostingRegressor(random_state=42))\n",
    "]\n",
    "\n",
    "meta_model = LinearRegression()\n",
    "\n",
    "stack_model = StackingRegressor(\n",
    "    estimators=base_models,\n",
    "    final_estimator=meta_model,\n",
    "    passthrough=True\n",
    ")\n",
    "\n",
    "stack_model.fit(X_train, y_train)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Evaluate Base Model\n",
    "# ----------------------------------------------------------\n",
    "stack_pred = stack_model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, stack_pred)\n",
    "rmse = sqrt(mean_squared_error(y_test, stack_pred))\n",
    "r2 = r2_score(y_test, stack_pred)\n",
    "\n",
    "print(\"\\n Ensemble Model Performance (Stacking Regressor):\")\n",
    "print(f\"MAE:  {mae:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"R²:   {r2:.4f}\")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Prediction Match Test\n",
    "# ----------------------------------------------------------\n",
    "def prediction_match_report(y_true, y_pred):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    abs_diff = np.abs(y_pred - y_true)\n",
    "    print(\"\\n=====  Phase 3 Prediction Match Test =====\")\n",
    "    print(f\"Exact Match Rate (<= $0.01): {(abs_diff<=0.01).mean():.4f}\")\n",
    "    print(f\"Close Match Rate (<= $1.00): {(abs_diff<=1.00).mean():.4f}\")\n",
    "    print(f\"±$5 Accuracy: {(abs_diff<=5.00).mean():.4f}\")\n",
    "\n",
    "prediction_match_report(y_test, stack_pred)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Residual Calibration\n",
    "# ----------------------------------------------------------\n",
    "from sklearn.ensemble import GradientBoostingRegressor as GBRResidual\n",
    "\n",
    "train_base_pred = stack_model.predict(X_train)\n",
    "train_residuals = y_train - train_base_pred\n",
    "\n",
    "residual_model = GBRResidual(\n",
    "    random_state=42,\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=2,\n",
    "    min_samples_leaf=20\n",
    ")\n",
    "\n",
    "residual_model.fit(X_train, train_residuals)\n",
    "\n",
    "test_final_pred = stack_pred + residual_model.predict(X_test)\n",
    "\n",
    "print(\"\\n=== Calibrated Ensemble Match Rates (Test) ===\")\n",
    "prediction_match_report(y_test, test_final_pred)\n"
   ],
   "id": "e8332bca987c1422"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ==========================================================\n",
    "# Phase 3 – Ensemble Learning (Stacking Regressor + PRD Logic + Residual Calibration)\n",
    "# ==========================================================\n",
    "\n",
    "from sklearn.ensemble import StackingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from math import sqrt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "# PRD-driven logic feature engineering\n",
    "# ---------------------------------------\n",
    "\n",
    "def add_prd_logic_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add PRD-driven behavioral features on top of Phase 2 engineered features.\n",
    "\n",
    "    Assumes df has:\n",
    "      - trip_duration_days\n",
    "      - miles_traveled\n",
    "      - total_receipts_amount\n",
    "      - cost_per_day\n",
    "      - cost_per_mile\n",
    "      - miles_per_day\n",
    "      - cost_ratio\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # --- Core derived features ---\n",
    "    # Spend per day\n",
    "    df[\"spend_per_day\"] = df[\"total_receipts_amount\"] / df[\"trip_duration_days\"].replace(0, np.nan)\n",
    "\n",
    "    # Log of miles (for non-linear mileage curve)\n",
    "    df[\"log_miles\"] = np.log1p(df[\"miles_traveled\"])\n",
    "\n",
    "    # Cents pattern in receipts (rounding quirks)\n",
    "    df[\"receipt_cents\"] = ((df[\"total_receipts_amount\"] * 100).round().astype(int) % 100)\n",
    "\n",
    "    # Efficiency score: active only in sweet-spot duration 4–6 days\n",
    "    df[\"efficiency_score\"] = df[\"miles_per_day\"] * df[\"trip_duration_days\"].between(4, 6).astype(int)\n",
    "\n",
    "    # --- Duration effects: bonuses / penalties ---\n",
    "    # 5-day bonus, >7-day penalty (vacation penalty)\n",
    "    df[\"duration_bonus_flag\"] = (df[\"trip_duration_days\"] == 5).astype(int)\n",
    "    df[\"long_trip_penalty_flag\"] = (df[\"trip_duration_days\"] > 7).astype(int)\n",
    "\n",
    "    # --- Mileage per day sweet spot and penalties ---\n",
    "    df[\"sweetspot_miles_per_day\"] = df[\"miles_per_day\"].between(180, 220).astype(int)\n",
    "    df[\"overshoot_miles_per_day\"] = (df[\"miles_per_day\"] > 300).astype(int)\n",
    "\n",
    "    # --- Spending rate: optimal band and penalties ---\n",
    "    df[\"optimal_spend_band\"] = df[\"spend_per_day\"].between(75, 120).astype(int)\n",
    "    df[\"low_spend_penalty\"] = (df[\"spend_per_day\"] < 50).astype(int)\n",
    "    df[\"high_spend_penalty\"] = (df[\"spend_per_day\"] > 120).astype(int)\n",
    "\n",
    "    # --- Receipt non-linearity around total receipts ---\n",
    "    # Approx: best around 600–800; penalties for very low/high\n",
    "    df[\"receipts_optimal_band\"] = df[\"total_receipts_amount\"].between(600, 800).astype(int)\n",
    "    df[\"receipts_outside_band\"] = (\n",
    "        (df[\"total_receipts_amount\"] < 50) | (df[\"total_receipts_amount\"] > 1000)\n",
    "    ).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "# Load Phase 2 dataset and add PRD features\n",
    "# ---------------------------------------\n",
    "\n",
    "combined_df = pd.read_csv(\"phase2_features_baseline_models.csv\")\n",
    "print(\"Dataset loaded for Phase 3!\")\n",
    "print(\"Shape:\", combined_df.shape)\n",
    "\n",
    "combined_df = add_prd_logic_features(combined_df)\n",
    "print(\"PRD-driven logic features added. New shape:\", combined_df.shape)\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "# Select Features and Target\n",
    "# ---------------------------------------\n",
    "features = [\n",
    "    # Original continuous features\n",
    "    \"trip_duration_days\",\n",
    "    \"miles_traveled\",\n",
    "    \"total_receipts_amount\",\n",
    "    \"cost_per_day\",\n",
    "    \"cost_per_mile\",\n",
    "    \"miles_per_day\",\n",
    "    \"cost_ratio\",\n",
    "\n",
    "    # PRD-driven features\n",
    "    \"spend_per_day\",\n",
    "    \"log_miles\",\n",
    "    \"receipt_cents\",\n",
    "    \"efficiency_score\",\n",
    "    \"duration_bonus_flag\",\n",
    "    \"long_trip_penalty_flag\",\n",
    "    \"sweetspot_miles_per_day\",\n",
    "    \"overshoot_miles_per_day\",\n",
    "    \"optimal_spend_band\",\n",
    "    \"low_spend_penalty\",\n",
    "    \"high_spend_penalty\",\n",
    "    \"receipts_optimal_band\",\n",
    "    \"receipts_outside_band\",\n",
    "]\n",
    "\n",
    "target = \"reimbursement\"\n",
    "\n",
    "X = combined_df[features]\n",
    "y = combined_df[target]\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "# Manual 75/25 split (to match earlier phases)\n",
    "# ---------------------------------------\n",
    "split = int(0.75 * len(combined_df))\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "print(f\"Training Samples: {len(X_train)}\")\n",
    "print(f\"Testing Samples:  {len(X_test)}\")\n",
    "print(\" Training Ensemble Model...\")\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "# Build Stacking Regressor\n",
    "# ---------------------------------------\n",
    "base_models = [\n",
    "    ('decision_tree', DecisionTreeRegressor(\n",
    "    random_state=42,\n",
    "    max_depth=5,\n",
    "    min_samples_leaf=20,\n",
    "    min_samples_split=40\n",
    ")),\n",
    "    ('random_forest', RandomForestRegressor(\n",
    "    n_estimators=400,        # more trees = smoother average\n",
    "    random_state=42,\n",
    "    max_depth=None,         # or try 12–16 if you see overfitting\n",
    "    min_samples_leaf=5,     # avoid ultra-tiny leaves\n",
    "    max_features=\"sqrt\",    # default, but good; could try 0.5 too\n",
    "    n_jobs=-1               # if you want speed-up\n",
    ")),\n",
    "    ('gradient_boosting', GradientBoostingRegressor(\n",
    "    random_state=42,\n",
    "    n_estimators=250,      # more boosting rounds\n",
    "    learning_rate=0.03,   # smaller steps\n",
    "    max_depth=3,          # more complex interactions\n",
    "    min_samples_leaf=10\n",
    "))\n",
    "]\n",
    "\n",
    "meta_model = LinearRegression()\n",
    "\n",
    "stack_model = StackingRegressor(\n",
    "    estimators=base_models,\n",
    "    final_estimator=meta_model,\n",
    "    passthrough=True,   # include original features along with base model preds\n",
    ")\n",
    "\n",
    "# Train Stacking Ensemble\n",
    "stack_model.fit(X_train, y_train)\n",
    "\n",
    "# Base predictions on test\n",
    "stack_pred = stack_model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, stack_pred)\n",
    "rmse = sqrt(mean_squared_error(y_test, stack_pred))\n",
    "r2 = r2_score(y_test, stack_pred)\n",
    "\n",
    "print(\"\\n Ensemble Model Performance (Stacking Regressor):\")\n",
    "print(f\"MAE:  {mae:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"R²:   {r2:.4f}\")\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "# Plot Actual vs Predicted (base ensemble)\n",
    "# ---------------------------------------\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.scatterplot(x=y_test, y=stack_pred, alpha=0.7)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.xlabel(\"Actual Reimbursement\")\n",
    "plt.ylabel(\"Predicted Reimbursement (Ensemble)\")\n",
    "plt.title(\"Actual vs Predicted — Stacking Ensemble (Base)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "# Match-rate helper\n",
    "# ---------------------------------------\n",
    "def prediction_match_report(y_true, y_pred, title=\"Phase 3 Prediction Match Test\"):\n",
    "    \"\"\"\n",
    "    Inline prediction match test.\n",
    "\n",
    "    - Exact matches: |pred - actual| <= $0.01\n",
    "    - Close matches: |pred - actual| <= $1.00\n",
    "    - ±$5 Accuracy:  |pred - actual| <= $5.00\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    abs_diff = np.abs(y_pred - y_true)\n",
    "\n",
    "    exact = (abs_diff <= 0.01).sum() / len(y_true)\n",
    "    close = (abs_diff <= 1.00).sum() / len(y_true)\n",
    "    within_5 = (abs_diff <= 5.00).sum() / len(y_true)\n",
    "\n",
    "    print(f\"\\n=====  {title} =====\")\n",
    "    print(f\"Exact Match Rate (<= $0.01): {exact:.4f}\")\n",
    "    print(f\"Close Match Rate (<= $1.00): {close:.4f}\")\n",
    "    print(f\"±$5 Accuracy: {within_5:.4f}\")\n",
    "\n",
    "\n",
    "# Base ensemble match rates\n",
    "prediction_match_report(y_test, stack_pred, title=\"Base Ensemble Match Rates (Test)\")\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "# Residual Calibration – Learn Corrections on Top of Ensemble\n",
    "# ---------------------------------------\n",
    "\n",
    "# 1) Compute base predictions on TRAIN and TEST\n",
    "train_base_pred = stack_model.predict(X_train)\n",
    "test_base_pred = stack_pred  # already computed on X_test\n",
    "\n",
    "# 2) Compute training residuals\n",
    "train_residuals = y_train - train_base_pred\n",
    "abs_res = np.abs(train_residuals)\n",
    "\n",
    "# 3) Build sample weights to emphasize large residuals\n",
    "sample_weights = np.ones_like(abs_res, dtype=float)\n",
    "sample_weights[abs_res > 1.0] *= 3.0   # more weight for errors > $1\n",
    "sample_weights[abs_res > 5.0] *= 5.0   # even more for errors > $5\n",
    "\n",
    "# 4) Extended feature space for residual model: [X, base_pred]\n",
    "X_train_res = X_train.copy()\n",
    "X_train_res[\"base_pred\"] = train_base_pred\n",
    "\n",
    "X_test_res = X_test.copy()\n",
    "X_test_res[\"base_pred\"] = test_base_pred\n",
    "\n",
    "# 5) Residual model (small gradient boosting regressor)\n",
    "residual_model = GradientBoostingRegressor(\n",
    "    random_state=42,\n",
    "    n_estimators=150,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=2,\n",
    "    min_samples_leaf=10,\n",
    ")\n",
    "\n",
    "residual_model.fit(X_train_res, train_residuals, sample_weight=sample_weights)\n",
    "print(\"\\n Residual correction model trained!\")\n",
    "\n",
    "\n",
    "# 6) Get correction predictions for train and test\n",
    "train_corrections = residual_model.predict(X_train_res)\n",
    "test_corrections = residual_model.predict(X_test_res)\n",
    "\n",
    "# 7) Calibrated final predictions\n",
    "train_final_pred = train_base_pred + train_corrections\n",
    "test_final_pred = test_base_pred + test_corrections\n",
    "\n",
    "# 8) Evaluate calibrated model (raw floats)\n",
    "calib_mae = mean_absolute_error(y_test, test_final_pred)\n",
    "calib_rmse = sqrt(mean_squared_error(y_test, test_final_pred))\n",
    "calib_r2 = r2_score(y_test, test_final_pred)\n",
    "\n",
    "print(\"\\n  Calibrated Ensemble Performance (with Residual Model):\")\n",
    "print(f\"MAE:  {calib_mae:.4f}\")\n",
    "print(f\"RMSE: {calib_rmse:.4f}\")\n",
    "print(f\"R²:   {calib_r2:.4f}\")\n",
    "\n",
    "# 9) Match-rate diagnostics: base vs calibrated\n",
    "prediction_match_report(y_test, test_base_pred, title=\"Base Ensemble Match Rates (Test)\")\n",
    "prediction_match_report(y_test, test_final_pred, title=\"Calibrated Ensemble Match Rates (Test – Raw)\")\n",
    "\n",
    "\n",
    "# 10) Optional: enforce money-like rounding (2 decimal cents)\n",
    "test_final_pred_money = np.round(test_final_pred, 2)\n",
    "prediction_match_report(y_test, test_final_pred_money,\n",
    "                        title=\"Calibrated Ensemble Match Rates (Test – Rounded to Cents)\")\n",
    "\n",
    "\n",
    "# 11) (Optional) See how calibration behaves on training set\n",
    "prediction_match_report(y_train, train_final_pred, title=\"Calibrated Ensemble Match Rates (Train)\")\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "dd74038e5099cf55"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ==========================================================\n",
    "# Phase 3 – Ensemble Learning (Stacking Regressor)\n",
    "# With ONLY Original + PRD Logic Features (NO rule flags)\n",
    "# ==========================================================\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import (\n",
    "    StackingRegressor,\n",
    "    RandomForestRegressor,\n",
    "    GradientBoostingRegressor,\n",
    "    VotingRegressor,\n",
    ")\n",
    "from sklearn.tree import DecisionTreeRegressor, export_text\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# ---------------------------------------\n",
    "# PRD-driven feature engineering\n",
    "# ---------------------------------------\n",
    "\n",
    "def add_prd_logic_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds PRD-derived behavioral features:\n",
    "      - spend_per_day curved behavior\n",
    "      - efficiency reward band (4–6 days & 180–220 mi/day)\n",
    "      - log miles decay\n",
    "      - receipt cents anomalies (.49 / .99)\n",
    "      - receipts curve near $700 (bonus)\n",
    "      - extreme receipt conditions\n",
    "\n",
    "    Assumes Phase 2 features already exist:\n",
    "        cost_per_day, miles_traveled, trip_duration_days, total_receipts_amount\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # === Core Derived Behavior ===\n",
    "    # Per-day spend: in Phase 2, cost_per_day already behaves like spend_per_day\n",
    "    df[\"spend_per_day\"] = df[\"cost_per_day\"]\n",
    "\n",
    "    # Miles per day with safe denominator\n",
    "    df[\"miles_per_day_safe\"] = (\n",
    "        df[\"miles_traveled\"] / df[\"trip_duration_days\"].clip(lower=1)\n",
    "    )\n",
    "\n",
    "    # Log miles for non-linear mileage curve\n",
    "    df[\"log_miles_traveled\"] = np.log1p(df[\"miles_traveled\"])\n",
    "\n",
    "    # === Receipt cents anomalies (.49 / .99 quirk) ===\n",
    "    df[\"receipt_cents\"] = np.round(\n",
    "        df[\"total_receipts_amount\"] - np.floor(df[\"total_receipts_amount\"]), 2\n",
    "    )\n",
    "    df[\"receipt_is_point_49_or_99\"] = df[\"receipt_cents\"].isin([0.49, 0.99]).astype(int)\n",
    "\n",
    "    # === Spend per day optimal band (75–120) & penalties ===\n",
    "    df[\"spend_per_day_good_band\"] = df[\"spend_per_day\"].between(75, 120).astype(int)\n",
    "    df[\"spend_per_day_low\"] = (df[\"spend_per_day\"] < 50).astype(int)\n",
    "    df[\"spend_per_day_high\"] = (df[\"spend_per_day\"] > 120).astype(int)\n",
    "\n",
    "    # === Receipt non-linearity around ~700 ===\n",
    "    df[\"receipts_near_700\"] = df[\"total_receipts_amount\"].between(600, 800).astype(int)\n",
    "    df[\"receipts_very_low\"] = (df[\"total_receipts_amount\"] < 50).astype(int)\n",
    "    df[\"receipts_very_high\"] = (df[\"total_receipts_amount\"] > 1000).astype(int)\n",
    "\n",
    "    # === Efficiency Sweet Spot (4–6 days ∧ 180–220 mi/day) ===\n",
    "    duration_mask = df[\"trip_duration_days\"].between(4, 6)\n",
    "    miles_mask = df[\"miles_per_day_safe\"].between(180, 220)\n",
    "\n",
    "    df[\"is_efficiency_sweet_spot\"] = (duration_mask & miles_mask).astype(int)\n",
    "    df[\"efficiency_score\"] = (\n",
    "        df[\"miles_per_day_safe\"] * df[\"is_efficiency_sweet_spot\"]\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Load Phase 2 dataset + add PRD features\n",
    "# ----------------------------------------------------------\n",
    "combined_df = pd.read_csv(\"phase2_features_baseline_models.csv\")\n",
    "\n",
    "print(\"Dataset loaded for Phase 3!\")\n",
    "print(\"Shape:\", combined_df.shape)\n",
    "\n",
    "combined_df = add_prd_logic_features(combined_df)\n",
    "print(\"PRD-driven logic features added. New shape:\", combined_df.shape)\n",
    "\n",
    "# ---------------------------------------\n",
    "# Feature Selection (ONLY original + PRD)\n",
    "# ---------------------------------------\n",
    "features = [\n",
    "    # Phase 2 continuous engineered inputs\n",
    "    \"trip_duration_days\",\n",
    "    \"miles_traveled\",\n",
    "    \"total_receipts_amount\",\n",
    "    \"cost_per_day\",\n",
    "    \"cost_per_mile\",\n",
    "    \"miles_per_day\",\n",
    "    \"cost_ratio\",\n",
    "\n",
    "    # PRD logic features\n",
    "    \"spend_per_day\",\n",
    "    \"miles_per_day_safe\",\n",
    "    \"log_miles_traveled\",\n",
    "    \"receipt_cents\",\n",
    "    \"receipt_is_point_49_or_99\",\n",
    "    \"spend_per_day_good_band\",\n",
    "    \"spend_per_day_low\",\n",
    "    \"spend_per_day_high\",\n",
    "    \"receipts_near_700\",\n",
    "    \"receipts_very_low\",\n",
    "    \"receipts_very_high\",\n",
    "    \"efficiency_score\",\n",
    "    \"is_efficiency_sweet_spot\",\n",
    "]\n",
    "\n",
    "target = \"reimbursement\"\n",
    "\n",
    "X = combined_df[features]\n",
    "y = combined_df[target]\n",
    "\n",
    "# ---------------------------------------\n",
    "# Manual 75/25 split\n",
    "# ---------------------------------------\n",
    "split = int(0.75 * len(combined_df))\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "print(f\"Training Samples: {len(X_train)}\")\n",
    "print(f\"Testing Samples:  {len(X_test)}\")\n",
    "print(\" Training Ensemble Model...\")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Define Base & Stacking\n",
    "# ----------------------------------------------------------\n",
    "base_models = [\n",
    "    (\"decision_tree\", DecisionTreeRegressor(random_state=42)),\n",
    "    (\"random_forest\", RandomForestRegressor(n_estimators=200, random_state=42)),\n",
    "    (\"gradient_boosting\", GradientBoostingRegressor(random_state=42)),\n",
    "]\n",
    "\n",
    "meta_model = LinearRegression()\n",
    "\n",
    "stack_model = StackingRegressor(\n",
    "    estimators=base_models,\n",
    "    final_estimator=meta_model,\n",
    "    passthrough=True,\n",
    ")\n",
    "\n",
    "stack_model.fit(X_train, y_train)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Evaluate Base Stacked Model\n",
    "# ----------------------------------------------------------\n",
    "stack_pred = stack_model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, stack_pred)\n",
    "rmse = sqrt(mean_squared_error(y_test, stack_pred))\n",
    "r2 = r2_score(y_test, stack_pred)\n",
    "\n",
    "print(\"\\n Ensemble Model Performance (Stacking Regressor):\")\n",
    "print(f\"MAE:  {mae:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"R²:   {r2:.4f}\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Prediction Match Test helper\n",
    "# ----------------------------------------------------------\n",
    "def prediction_match_report(y_true, y_pred, label=\"Phase 3 Prediction Match Test\"):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    abs_diff = np.abs(y_pred - y_true)\n",
    "    print(f\"\\n=====  {label} =====\")\n",
    "    print(f\"Exact Match Rate (<= $0.01): {(abs_diff <= 0.01).mean():.4f}\")\n",
    "    print(f\"Close Match Rate (<= $1.00): {(abs_diff <= 1.00).mean():.4f}\")\n",
    "    print(f\"±$5 Accuracy: {(abs_diff <= 5.00).mean():.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\n=====  Base Ensemble Match Rates (Test) =====\")\n",
    "prediction_match_report(y_test, stack_pred, label=\"Base Ensemble Match Rates (Test)\")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Residual Calibration\n",
    "# ----------------------------------------------------------\n",
    "from sklearn.ensemble import GradientBoostingRegressor as GBRResidual\n",
    "\n",
    "train_base_pred = stack_model.predict(X_train)\n",
    "train_residuals = y_train - train_base_pred\n",
    "\n",
    "residual_model = GBRResidual(\n",
    "    random_state=42,\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=2,\n",
    "    min_samples_leaf=20,\n",
    ")\n",
    "\n",
    "residual_model.fit(X_train, train_residuals)\n",
    "\n",
    "print(\"\\n Residual correction model trained!\")\n",
    "\n",
    "# Corrections\n",
    "train_corrections = residual_model.predict(X_train)\n",
    "test_corrections = residual_model.predict(X_test)\n",
    "\n",
    "# Calibrated predictions\n",
    "train_final_pred = train_base_pred + train_corrections\n",
    "test_final_pred = stack_pred + test_corrections\n",
    "\n",
    "# Calibrated metrics\n",
    "calib_mae = mean_absolute_error(y_test, test_final_pred)\n",
    "calib_rmse = sqrt(mean_squared_error(y_test, test_final_pred))\n",
    "calib_r2 = r2_score(y_test, test_final_pred)\n",
    "\n",
    "print(\"\\n  Calibrated Ensemble Performance (with Residual Model):\")\n",
    "print(f\"MAE:  {calib_mae:.4f}\")\n",
    "print(f\"RMSE: {calib_rmse:.4f}\")\n",
    "print(f\"R²:   {calib_r2:.4f}\")\n",
    "\n",
    "print(\"\\n=====  Calibrated Ensemble Match Rates (Test – Raw) =====\")\n",
    "prediction_match_report(y_test, test_final_pred, label=\"Calibrated Ensemble Match Rates (Test – Raw)\")\n",
    "\n",
    "# Optional: rounded to cents\n",
    "test_final_pred_rounded = np.round(test_final_pred, 2)\n",
    "print(\"\\n=====  Calibrated Ensemble Match Rates (Test – Rounded to Cents) =====\")\n",
    "prediction_match_report(\n",
    "    y_test, test_final_pred_rounded, label=\"Calibrated Ensemble Match Rates (Test – Rounded to Cents)\"\n",
    ")\n",
    "\n",
    "print(\"\\n=====  Calibrated Ensemble Match Rates (Train) =====\")\n",
    "prediction_match_report(y_train, train_final_pred, label=\"Calibrated Ensemble Match Rates (Train)\")\n",
    "\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# Advanced Techniques – SVR, MLP, Voting Ensemble\n",
    "# ==========================================================\n",
    "\n",
    "# --- Support Vector Regression (SVR) ---\n",
    "svr_model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    SVR(kernel=\"rbf\", C=10.0, epsilon=5.0),\n",
    ")\n",
    "\n",
    "svr_model.fit(X_train, y_train)\n",
    "svr_pred = svr_model.predict(X_test)\n",
    "\n",
    "svr_mae = mean_absolute_error(y_test, svr_pred)\n",
    "svr_rmse = sqrt(mean_squared_error(y_test, svr_pred))\n",
    "svr_r2 = r2_score(y_test, svr_pred)\n",
    "\n",
    "print(\"\\n SVR Performance:\")\n",
    "print(f\"MAE:  {svr_mae:.4f}\")\n",
    "print(f\"RMSE: {svr_rmse:.4f}\")\n",
    "print(f\"R²:   {svr_r2:.4f}\")\n",
    "prediction_match_report(y_test, svr_pred, label=\"SVR Match Rates (Test)\")\n",
    "\n",
    "\n",
    "# --- Neural Network (MLPRegressor) ---\n",
    "mlp_model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    MLPRegressor(\n",
    "        hidden_layer_sizes=(64, 32),\n",
    "        activation=\"relu\",\n",
    "        solver=\"adam\",\n",
    "        alpha=1e-3,\n",
    "        max_iter=2000,\n",
    "        random_state=42,\n",
    "    ),\n",
    ")\n",
    "\n",
    "mlp_model.fit(X_train, y_train)\n",
    "mlp_pred = mlp_model.predict(X_test)\n",
    "\n",
    "mlp_mae = mean_absolute_error(y_test, mlp_pred)\n",
    "mlp_rmse = sqrt(mean_squared_error(y_test, mlp_pred))\n",
    "mlp_r2 = r2_score(y_test, mlp_pred)\n",
    "\n",
    "print(\"\\n MLP Performance:\")\n",
    "print(f\"MAE:  {mlp_mae:.4f}\")\n",
    "print(f\"RMSE: {mlp_rmse:.4f}\")\n",
    "print(f\"R²:   {mlp_r2:.4f}\")\n",
    "prediction_match_report(y_test, mlp_pred, label=\"MLP Match Rates (Test)\")\n",
    "\n",
    "\n",
    "# --- Voting Regressor (simple averaging ensemble) ---\n",
    "voting_reg = VotingRegressor(\n",
    "    estimators=[\n",
    "        (\"rf\", base_models[1][1]),   # RandomForestRegressor\n",
    "        (\"gbr\", base_models[2][1]),  # GradientBoostingRegressor\n",
    "        (\"tree\", base_models[0][1]), # DecisionTreeRegressor\n",
    "    ]\n",
    ")\n",
    "\n",
    "voting_reg.fit(X_train, y_train)\n",
    "voting_pred = voting_reg.predict(X_test)\n",
    "\n",
    "v_mae = mean_absolute_error(y_test, voting_pred)\n",
    "v_rmse = sqrt(mean_squared_error(y_test, voting_pred))\n",
    "v_r2 = r2_score(y_test, voting_pred)\n",
    "\n",
    "print(\"\\n Voting Regressor Performance:\")\n",
    "print(f\"MAE:  {v_mae:.4f}\")\n",
    "print(f\"RMSE: {v_rmse:.4f}\")\n",
    "print(f\"R²:   {v_r2:.4f}\")\n",
    "prediction_match_report(y_test, voting_pred, label=\"Voting Regressor Match Rates (Test)\")\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# Rule-Based Learning – Surrogate Tree for Interpretability\n",
    "# ==========================================================\n",
    "\n",
    "# Train a shallow tree to mimic the calibrated ensemble behavior\n",
    "surrogate_tree = DecisionTreeRegressor(\n",
    "    max_depth=4,\n",
    "    min_samples_leaf=20,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Use calibrated train predictions as the \"target\" for the surrogate\n",
    "surrogate_tree.fit(X_train, train_final_pred)\n",
    "\n",
    "tree_rules = export_text(surrogate_tree, feature_names=list(X.columns))\n",
    "\n",
    "print(\"\\n=== Surrogate Tree Rules (approximate learned logic) ===\")\n",
    "print(tree_rules)\n"
   ],
   "id": "40d3167a54b6a454"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ==========================================================\n",
    "# Phase 3 – Ensemble Learning (Stacking Regressor)\n",
    "# With ONLY Original + PRD Logic Features (NO rule flags)\n",
    "# ==========================================================\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import (\n",
    "    StackingRegressor,\n",
    "    RandomForestRegressor,\n",
    "    GradientBoostingRegressor,\n",
    "    VotingRegressor,\n",
    "    RandomForestClassifier,\n",
    ")\n",
    "from sklearn.tree import DecisionTreeRegressor, export_text\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# ---------------------------------------\n",
    "# PRD-driven feature engineering\n",
    "# ---------------------------------------\n",
    "\n",
    "def add_prd_logic_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds PRD-derived behavioral features:\n",
    "      - spend_per_day curved behavior\n",
    "      - efficiency reward band (4–6 days & 180–220 mi/day)\n",
    "      - log miles decay\n",
    "      - receipt cents anomalies (.49 / .99)\n",
    "      - receipts curve near $700 (bonus)\n",
    "      - extreme receipt conditions\n",
    "\n",
    "    Assumes Phase 2 features already exist:\n",
    "        cost_per_day, miles_traveled, trip_duration_days, total_receipts_amount\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # === Core Derived Behavior ===\n",
    "    # Per-day spend: in Phase 2, cost_per_day already behaves like spend_per_day\n",
    "    df[\"spend_per_day\"] = df[\"cost_per_day\"]\n",
    "\n",
    "    # Miles per day with safe denominator\n",
    "    df[\"miles_per_day_safe\"] = (\n",
    "        df[\"miles_traveled\"] / df[\"trip_duration_days\"].clip(lower=1)\n",
    "    )\n",
    "\n",
    "    # Log miles for non-linear mileage curve\n",
    "    df[\"log_miles_traveled\"] = np.log1p(df[\"miles_traveled\"])\n",
    "\n",
    "    # === Receipt cents anomalies (.49 / .99 quirk) ===\n",
    "    df[\"receipt_cents\"] = np.round(\n",
    "        df[\"total_receipts_amount\"] - np.floor(df[\"total_receipts_amount\"]), 2\n",
    "    )\n",
    "    df[\"receipt_is_point_49_or_99\"] = df[\"receipt_cents\"].isin([0.49, 0.99]).astype(int)\n",
    "\n",
    "    # === Spend per day optimal band (75–120) & penalties ===\n",
    "    df[\"spend_per_day_good_band\"] = df[\"spend_per_day\"].between(75, 120).astype(int)\n",
    "    df[\"spend_per_day_low\"] = (df[\"spend_per_day\"] < 50).astype(int)\n",
    "    df[\"spend_per_day_high\"] = (df[\"spend_per_day\"] > 120).astype(int)\n",
    "\n",
    "    # === Receipt non-linearity around ~700 ===\n",
    "    df[\"receipts_near_700\"] = df[\"total_receipts_amount\"].between(600, 800).astype(int)\n",
    "    df[\"receipts_very_low\"] = (df[\"total_receipts_amount\"] < 50).astype(int)\n",
    "    df[\"receipts_very_high\"] = (df[\"total_receipts_amount\"] > 1000).astype(int)\n",
    "\n",
    "    # === Efficiency Sweet Spot (4–6 days ∧ 180–220 mi/day) ===\n",
    "    duration_mask = df[\"trip_duration_days\"].between(4, 6)\n",
    "    miles_mask = df[\"miles_per_day_safe\"].between(180, 220)\n",
    "\n",
    "    df[\"is_efficiency_sweet_spot\"] = (duration_mask & miles_mask).astype(int)\n",
    "    df[\"efficiency_score\"] = (\n",
    "        df[\"miles_per_day_safe\"] * df[\"is_efficiency_sweet_spot\"]\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Load Phase 2 dataset + add PRD features\n",
    "# ----------------------------------------------------------\n",
    "combined_df = pd.read_csv(\"phase2_features_baseline_models.csv\")\n",
    "\n",
    "print(\"Dataset loaded for Phase 3!\")\n",
    "print(\"Shape:\", combined_df.shape)\n",
    "\n",
    "combined_df = add_prd_logic_features(combined_df)\n",
    "print(\"PRD-driven logic features added. New shape:\", combined_df.shape)\n",
    "\n",
    "# ---------------------------------------\n",
    "# Feature Selection (ONLY original + PRD)\n",
    "# ---------------------------------------\n",
    "features = [\n",
    "    # Phase 2 continuous engineered inputs\n",
    "    \"trip_duration_days\",\n",
    "    \"miles_traveled\",\n",
    "    \"total_receipts_amount\",\n",
    "    \"cost_per_day\",\n",
    "    \"cost_per_mile\",\n",
    "    \"miles_per_day\",\n",
    "    \"cost_ratio\",\n",
    "\n",
    "    # PRD logic features\n",
    "    \"spend_per_day\",\n",
    "    \"miles_per_day_safe\",\n",
    "    \"log_miles_traveled\",\n",
    "    \"receipt_cents\",\n",
    "    \"receipt_is_point_49_or_99\",\n",
    "    \"spend_per_day_good_band\",\n",
    "    \"spend_per_day_low\",\n",
    "    \"spend_per_day_high\",\n",
    "    \"receipts_near_700\",\n",
    "    \"receipts_very_low\",\n",
    "    \"receipts_very_high\",\n",
    "    \"efficiency_score\",\n",
    "    \"is_efficiency_sweet_spot\",\n",
    "]\n",
    "\n",
    "target = \"reimbursement\"\n",
    "\n",
    "X = combined_df[features]\n",
    "y = combined_df[target]\n",
    "# ==========================================================\n",
    "# Sanity Checks + Random (Shuffled) Train/Test Split\n",
    "# ==========================================================\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"\\n===== 🔍 Sanity Checks =====\")\n",
    "\n",
    "# 1) How quantized is reimbursement? (lots of repeated exact values suggests discrete payout bands)\n",
    "y_rounded = combined_df[target].round(2)\n",
    "n_unique = y_rounded.nunique()\n",
    "top_counts = y_rounded.value_counts().head(15)\n",
    "\n",
    "print(f\"Unique reimbursements (rounded to cents): {n_unique}\")\n",
    "print(\"Most common payouts (top 15):\")\n",
    "print(top_counts.to_string())\n",
    "\n",
    "# 2) Do identical feature rows map to multiple reimbursements? (hidden inputs/noise)\n",
    "tmp = combined_df[features + [target]].copy()\n",
    "tmp[target] = tmp[target].round(2)  # compare at cent-level\n",
    "\n",
    "# group by X and count unique y values per identical X row\n",
    "y_variation = tmp.groupby(features)[target].nunique()\n",
    "multi_y_count = int((y_variation > 1).sum())\n",
    "max_y_for_same_x = int(y_variation.max())\n",
    "\n",
    "print(f\"Identical X mapping to >1 reimbursement (cent-rounded): {multi_y_count}\")\n",
    "print(f\"Max distinct reimbursements for the same X: {max_y_for_same_x}\")\n",
    "\n",
    "if multi_y_count > 0:\n",
    "    print(\"⚠️  Warning: Same inputs sometimes produce different outputs.\")\n",
    "    print(\"    This suggests hidden variables and/or stochastic behavior, which limits match-rate ceiling.\")\n",
    "\n",
    "# ==========================================================\n",
    "# Random / Shuffled split (recommended vs first-75% / last-25%)\n",
    "# ==========================================================\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.25,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(\"\\n===== ✅ Random Split Applied =====\")\n",
    "print(f\"Training Samples: {len(X_train)}\")\n",
    "print(f\"Testing Samples:  {len(X_test)}\")\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Define Base & Stacking\n",
    "# ----------------------------------------------------------\n",
    "base_models = [\n",
    "    (\"decision_tree\", DecisionTreeRegressor(random_state=42)),\n",
    "    (\"random_forest\", RandomForestRegressor(n_estimators=200, random_state=42)),\n",
    "    (\"gradient_boosting\", GradientBoostingRegressor(random_state=42)),\n",
    "]\n",
    "\n",
    "meta_model = LinearRegression()\n",
    "\n",
    "stack_model = StackingRegressor(\n",
    "    estimators=base_models,\n",
    "    final_estimator=meta_model,\n",
    "    passthrough=True,\n",
    ")\n",
    "\n",
    "stack_model.fit(X_train, y_train)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Evaluate Base Stacked Model\n",
    "# ----------------------------------------------------------\n",
    "stack_pred = stack_model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, stack_pred)\n",
    "rmse = sqrt(mean_squared_error(y_test, stack_pred))\n",
    "r2 = r2_score(y_test, stack_pred)\n",
    "\n",
    "print(\"\\n Ensemble Model Performance (Stacking Regressor):\")\n",
    "print(f\"MAE:  {mae:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"R²:   {r2:.4f}\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Prediction Match Test helper\n",
    "# ----------------------------------------------------------\n",
    "def prediction_match_report(y_true, y_pred, label=\"Phase 3 Prediction Match Test\"):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    abs_diff = np.abs(y_pred - y_true)\n",
    "    print(f\"\\n=====  {label} =====\")\n",
    "    print(f\"Exact Match Rate (<= $0.01): {(abs_diff <= 0.01).mean():.4f}\")\n",
    "    print(f\"Close Match Rate (<= $1.00): {(abs_diff <= 1.00).mean():.4f}\")\n",
    "    print(f\"±$5 Accuracy: {(abs_diff <= 5.00).mean():.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\n=====  Base Ensemble Match Rates (Test) =====\")\n",
    "prediction_match_report(y_test, stack_pred, label=\"Base Ensemble Match Rates (Test)\")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Residual Calibration\n",
    "# ----------------------------------------------------------\n",
    "from sklearn.ensemble import GradientBoostingRegressor as GBRResidual\n",
    "\n",
    "train_base_pred = stack_model.predict(X_train)\n",
    "train_residuals = y_train - train_base_pred\n",
    "\n",
    "residual_model = GBRResidual(\n",
    "    random_state=42,\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=2,\n",
    "    min_samples_leaf=20,\n",
    ")\n",
    "\n",
    "residual_model.fit(X_train, train_residuals)\n",
    "\n",
    "print(\"\\n Residual correction model trained!\")\n",
    "\n",
    "# Corrections\n",
    "train_corrections = residual_model.predict(X_train)\n",
    "test_corrections = residual_model.predict(X_test)\n",
    "\n",
    "# Calibrated predictions\n",
    "train_final_pred = train_base_pred + train_corrections\n",
    "test_final_pred = stack_pred + test_corrections\n",
    "\n",
    "# Calibrated metrics\n",
    "calib_mae = mean_absolute_error(y_test, test_final_pred)\n",
    "calib_rmse = sqrt(mean_squared_error(y_test, test_final_pred))\n",
    "calib_r2 = r2_score(y_test, test_final_pred)\n",
    "\n",
    "print(\"\\n  Calibrated Ensemble Performance (with Residual Model):\")\n",
    "print(f\"MAE:  {calib_mae:.4f}\")\n",
    "print(f\"RMSE: {calib_rmse:.4f}\")\n",
    "print(f\"R²:   {calib_r2:.4f}\")\n",
    "\n",
    "print(\"\\n=====  Calibrated Ensemble Match Rates (Test – Raw) =====\")\n",
    "prediction_match_report(y_test, test_final_pred, label=\"Calibrated Ensemble Match Rates (Test – Raw)\")\n",
    "\n",
    "# Optional: rounded to cents\n",
    "test_final_pred_rounded = np.round(test_final_pred, 2)\n",
    "print(\"\\n=====  Calibrated Ensemble Match Rates (Test – Rounded to Cents) =====\")\n",
    "prediction_match_report(\n",
    "    y_test, test_final_pred_rounded, label=\"Calibrated Ensemble Match Rates (Test – Rounded to Cents)\"\n",
    ")\n",
    "\n",
    "print(\"\\n=====  Calibrated Ensemble Match Rates (Train) =====\")\n",
    "prediction_match_report(y_train, train_final_pred, label=\"Calibrated Ensemble Match Rates (Train)\")\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# Advanced Techniques – SVR, MLP, Voting Ensemble\n",
    "# ==========================================================\n",
    "\n",
    "# --- Support Vector Regression (SVR) ---\n",
    "svr_model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    SVR(kernel=\"rbf\", C=10.0, epsilon=5.0),\n",
    ")\n",
    "\n",
    "svr_model.fit(X_train, y_train)\n",
    "svr_pred = svr_model.predict(X_test)\n",
    "\n",
    "svr_mae = mean_absolute_error(y_test, svr_pred)\n",
    "svr_rmse = sqrt(mean_squared_error(y_test, svr_pred))\n",
    "svr_r2 = r2_score(y_test, svr_pred)\n",
    "\n",
    "print(\"\\n SVR Performance:\")\n",
    "print(f\"MAE:  {svr_mae:.4f}\")\n",
    "print(f\"RMSE: {svr_rmse:.4f}\")\n",
    "print(f\"R²:   {svr_r2:.4f}\")\n",
    "prediction_match_report(y_test, svr_pred, label=\"SVR Match Rates (Test)\")\n",
    "\n",
    "\n",
    "# --- Neural Network (MLPRegressor) ---\n",
    "mlp_model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    MLPRegressor(\n",
    "        hidden_layer_sizes=(64, 32),\n",
    "        activation=\"relu\",\n",
    "        solver=\"adam\",\n",
    "        alpha=1e-3,\n",
    "        max_iter=2000,\n",
    "        random_state=42,\n",
    "    ),\n",
    ")\n",
    "\n",
    "mlp_model.fit(X_train, y_train)\n",
    "mlp_pred = mlp_model.predict(X_test)\n",
    "\n",
    "mlp_mae = mean_absolute_error(y_test, mlp_pred)\n",
    "mlp_rmse = sqrt(mean_squared_error(y_test, mlp_pred))\n",
    "mlp_r2 = r2_score(y_test, mlp_pred)\n",
    "\n",
    "print(\"\\n MLP Performance:\")\n",
    "print(f\"MAE:  {mlp_mae:.4f}\")\n",
    "print(f\"RMSE: {mlp_rmse:.4f}\")\n",
    "print(f\"R²:   {mlp_r2:.4f}\")\n",
    "prediction_match_report(y_test, mlp_pred, label=\"MLP Match Rates (Test)\")\n",
    "\n",
    "\n",
    "# --- Voting Regressor (simple averaging ensemble) ---\n",
    "voting_reg = VotingRegressor(\n",
    "    estimators=[\n",
    "        (\"rf\", base_models[1][1]),   # RandomForestRegressor\n",
    "        (\"gbr\", base_models[2][1]),  # GradientBoostingRegressor\n",
    "        (\"tree\", base_models[0][1]), # DecisionTreeRegressor\n",
    "    ]\n",
    ")\n",
    "\n",
    "voting_reg.fit(X_train, y_train)\n",
    "voting_pred = voting_reg.predict(X_test)\n",
    "\n",
    "v_mae = mean_absolute_error(y_test, voting_pred)\n",
    "v_rmse = sqrt(mean_squared_error(y_test, voting_pred))\n",
    "v_r2 = r2_score(y_test, voting_pred)\n",
    "\n",
    "print(\"\\n Voting Regressor Performance:\")\n",
    "print(f\"MAE:  {v_mae:.4f}\")\n",
    "print(f\"RMSE: {v_rmse:.4f}\")\n",
    "print(f\"R²:   {v_r2:.4f}\")\n",
    "prediction_match_report(y_test, voting_pred, label=\"Voting Regressor Match Rates (Test)\")\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# Rule-Based Learning – Surrogate Tree for Interpretability\n",
    "# ==========================================================\n",
    "\n",
    "# Train a shallow tree to mimic the calibrated ensemble behavior\n",
    "surrogate_tree = DecisionTreeRegressor(\n",
    "    max_depth=4,\n",
    "    min_samples_leaf=20,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Use calibrated train predictions as the \"target\" for the surrogate\n",
    "surrogate_tree.fit(X_train, train_final_pred)\n",
    "\n",
    "tree_rules = export_text(surrogate_tree, feature_names=list(X.columns))\n",
    "\n",
    "print(\"\\n=== Surrogate Tree Rules (approximate learned logic) ===\")\n",
    "print(tree_rules)\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# DISCRETE / TIERED MODELING – Classification over Payout Buckets (FIXED for shuffled split)\n",
    "# ==========================================================\n",
    "\n",
    "# 1) Discretize reimbursement into buckets (e.g., $5 tiers)\n",
    "bucket_size = 5.0  # try 2.0 or 1.0 for finer tiers (but may overfit)\n",
    "\n",
    "# Build bucket labels on the full dataset, then align by index to train/test\n",
    "y_bucket_all = (combined_df[target] / bucket_size).round().astype(int)\n",
    "\n",
    "y_bucket_train = y_bucket_all.loc[y_train.index]\n",
    "y_bucket_test  = y_bucket_all.loc[y_test.index]\n",
    "\n",
    "# 2) Map from bucket -> representative payout value (median payout in TRAIN for that bucket)\n",
    "bucket_to_median = {}\n",
    "for b in np.sort(y_bucket_train.unique()):\n",
    "    mask = (y_bucket_train == b).values  # aligned to y_train order\n",
    "    bucket_to_median[int(b)] = float(np.median(y_train.values[mask]))\n",
    "\n",
    "def buckets_to_payouts(bucket_array):\n",
    "    \"\"\"Convert bucket labels to concrete payout values using TRAIN median per bucket.\"\"\"\n",
    "    bucket_array = np.asarray(bucket_array, dtype=int)\n",
    "    out = np.empty_like(bucket_array, dtype=float)\n",
    "    for i, b in enumerate(bucket_array):\n",
    "        out[i] = bucket_to_median.get(int(b), float(b) * bucket_size)  # fallback to bucket center\n",
    "    return out\n",
    "\n",
    "# 3) Train a classifier over payout buckets\n",
    "rf_cls = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=5,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "rf_cls.fit(X_train, y_bucket_train)\n",
    "\n",
    "# 4) Predict buckets on test + convert to payout values\n",
    "bucket_pred_test = rf_cls.predict(X_test)\n",
    "tier_pred_test = buckets_to_payouts(bucket_pred_test)\n",
    "\n",
    "# 5) Evaluate using same regression + match metrics\n",
    "tier_mae = mean_absolute_error(y_test, tier_pred_test)\n",
    "tier_rmse = sqrt(mean_squared_error(y_test, tier_pred_test))\n",
    "tier_r2 = r2_score(y_test, tier_pred_test)\n",
    "\n",
    "print(f\"\\n Tier Classifier (RandomForestClassifier over ${bucket_size:g} buckets) Performance:\")\n",
    "print(f\"MAE:  {tier_mae:.4f}\")\n",
    "print(f\"RMSE: {tier_rmse:.4f}\")\n",
    "print(f\"R²:   {tier_r2:.4f}\")\n",
    "prediction_match_report(y_test, tier_pred_test, label=\"Tier Classifier Match Rates (Test)\")\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# CASE-BASED REASONING – kNN over Historical Trips\n",
    "# ==========================================================\n",
    "\n",
    "# kNN in feature space: find k most similar training trips and use their payouts\n",
    "k_neighbors = 5\n",
    "nn = NearestNeighbors(n_neighbors=k_neighbors)\n",
    "nn.fit(X_train)\n",
    "\n",
    "distances, indices = nn.kneighbors(X_test)\n",
    "\n",
    "# indices shape: (n_test, k_neighbors)\n",
    "neighbor_reimb = y_train.values[indices]  # reimbursements of nearest neighbors\n",
    "\n",
    "# Several possible aggregation strategies; use median to stay robust\n",
    "knn_case_pred = np.median(neighbor_reimb, axis=1)\n",
    "\n",
    "knn_mae = mean_absolute_error(y_test, knn_case_pred)\n",
    "knn_rmse = sqrt(mean_squared_error(y_test, knn_case_pred))\n",
    "knn_r2 = r2_score(y_test, knn_case_pred)\n",
    "\n",
    "print(\"\\n kNN Case-Based Predictor Performance (k=5, median of neighbor reimbursements):\")\n",
    "print(f\"MAE:  {knn_mae:.4f}\")\n",
    "print(f\"RMSE: {knn_rmse:.4f}\")\n",
    "print(f\"R²:   {knn_r2:.4f}\")\n",
    "prediction_match_report(y_test, knn_case_pred, label=\"kNN Case-Based Match Rates (Test)\")\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "349a87328da7a89"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "THe combined system is strickly wrose that best model so far.",
   "id": "4e08c863d27dcc4d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ==========================================================\n",
    "# HYBRID META-MODEL: combine calibrated ensemble + tier classifier + kNN\n",
    "# ==========================================================\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "print(\"\\n\\n=====  Building Hybrid Meta-Model (Reg + Tier + kNN) =====\")\n",
    "\n",
    "# ---------- 1) Tiered classifier over payout buckets ----------\n",
    "bucket_size = 5.0  # you can try 2.0 or 1.0 for finer tiers\n",
    "\n",
    "combined_df[\"reimb_bucket\"] = (combined_df[\"reimbursement\"] / bucket_size).round().astype(int)\n",
    "\n",
    "y_bucket = combined_df[\"reimb_bucket\"]\n",
    "y_bucket_train = y_bucket[:split]\n",
    "y_bucket_test = y_bucket[split:]\n",
    "\n",
    "# Map: bucket -> median reimbursement in that bucket (using TRAIN data only)\n",
    "bucket_to_median = {}\n",
    "for b in np.unique(y_bucket_train):\n",
    "    mask = (y_bucket_train == b)\n",
    "    bucket_to_median[b] = float(np.median(y_train[mask]))\n",
    "\n",
    "def buckets_to_payouts(bucket_array):\n",
    "    vals = []\n",
    "    for b in bucket_array:\n",
    "        if b in bucket_to_median:\n",
    "            vals.append(bucket_to_median[b])\n",
    "        else:\n",
    "            vals.append(b * bucket_size)\n",
    "    return np.array(vals)\n",
    "\n",
    "rf_cls = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=5,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "rf_cls.fit(X_train, y_bucket_train)\n",
    "\n",
    "bucket_pred_train = rf_cls.predict(X_train)\n",
    "bucket_pred_test = rf_cls.predict(X_test)\n",
    "\n",
    "tier_pred_train = buckets_to_payouts(bucket_pred_train)\n",
    "tier_pred_test  = buckets_to_payouts(bucket_pred_test)\n",
    "\n",
    "print(\"\\n Tier classifier trained.\")\n",
    "\n",
    "# ---------- 2) kNN case-based prediction (train + test) ----------\n",
    "\n",
    "k_neighbors = 5\n",
    "\n",
    "# For TEST: neighbors from training set (already what we want)\n",
    "nn_test = NearestNeighbors(n_neighbors=k_neighbors)\n",
    "nn_test.fit(X_train)\n",
    "dist_test, idx_test = nn_test.kneighbors(X_test)\n",
    "neighbor_reimb_test = y_train.values[idx_test]\n",
    "knn_case_pred_test = np.median(neighbor_reimb_test, axis=1)\n",
    "\n",
    "# For TRAIN: use k+1 neighbors and drop self to avoid trivial zero distance\n",
    "nn_train = NearestNeighbors(n_neighbors=k_neighbors + 1)\n",
    "nn_train.fit(X_train)\n",
    "dist_train, idx_train = nn_train.kneighbors(X_train)\n",
    "# Drop the first column (self neighbor)\n",
    "neighbor_reimb_train = y_train.values[idx_train[:, 1:]]\n",
    "knn_case_pred_train = np.median(neighbor_reimb_train, axis=1)\n",
    "\n",
    "print(\" kNN case-based predictor trained.\")\n",
    "\n",
    "# ---------- 3) Build meta-features from three models ----------\n",
    "# Base calibrated ensemble outputs: train_final_pred and test_final_pred\n",
    "# (already computed earlier in your script)\n",
    "\n",
    "meta_X_train = np.column_stack([\n",
    "    train_final_pred,   # calibrated stack regressor\n",
    "    tier_pred_train,    # tier classifier regression\n",
    "    knn_case_pred_train # kNN case-based regression\n",
    "])\n",
    "meta_y_train = y_train.values\n",
    "\n",
    "meta_X_test = np.column_stack([\n",
    "    test_final_pred,\n",
    "    tier_pred_test,\n",
    "    knn_case_pred_test\n",
    "])\n",
    "\n",
    "# ---------- 4) Meta-learner: linear blend of the three ----------\n",
    "meta_blender = LinearRegression()\n",
    "meta_blender.fit(meta_X_train, meta_y_train)\n",
    "\n",
    "hybrid_pred_test = meta_blender.predict(meta_X_test)\n",
    "hybrid_pred_test_rounded = np.round(hybrid_pred_test, 2)\n",
    "\n",
    "hybrid_mae = mean_absolute_error(y_test, hybrid_pred_test)\n",
    "hybrid_rmse = sqrt(mean_squared_error(y_test, hybrid_pred_test))\n",
    "hybrid_r2 = r2_score(y_test, hybrid_pred_test)\n",
    "\n",
    "print(\"\\n Hybrid Meta-Model Performance (Reg + Tier + kNN):\")\n",
    "print(f\"MAE:  {hybrid_mae:.4f}\")\n",
    "print(f\"RMSE: {hybrid_rmse:.4f}\")\n",
    "print(f\"R²:   {hybrid_r2:.4f}\")\n",
    "\n",
    "print(\"\\n=====  Hybrid Meta-Model Match Rates (Test – Raw) =====\")\n",
    "prediction_match_report(y_test, hybrid_pred_test, label=\"Hybrid Meta-Model (Raw)\")\n",
    "\n",
    "print(\"\\n=====  Hybrid Meta-Model Match Rates (Test – Rounded to Cents) =====\")\n",
    "prediction_match_report(y_test, hybrid_pred_test_rounded, label=\"Hybrid Meta-Model (Rounded)\")\n"
   ],
   "id": "27be97ca2a631a87"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ==========================================================\n",
    "# Phase 3 – Ensemble Learning (Stacking Regressor)\n",
    "# With ONLY Original + PRD Logic Features (NO rule flags)\n",
    "# ==========================================================\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import (\n",
    "    StackingRegressor,\n",
    "    RandomForestRegressor,\n",
    "    GradientBoostingRegressor,\n",
    "    VotingRegressor,\n",
    "    RandomForestClassifier,\n",
    ")\n",
    "from sklearn.tree import DecisionTreeRegressor, export_text\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# ---------------------------------------\n",
    "# PRD-driven feature engineering\n",
    "# ---------------------------------------\n",
    "\n",
    "def add_prd_logic_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds PRD-derived behavioral features:\n",
    "      - spend_per_day curved behavior\n",
    "      - efficiency reward band (4–6 days & 180–220 mi/day)\n",
    "      - log miles decay\n",
    "      - receipt cents anomalies (.49 / .99)\n",
    "      - receipts curve near $700 (bonus)\n",
    "      - extreme receipt conditions\n",
    "\n",
    "    Assumes Phase 2 features already exist:\n",
    "        cost_per_day, miles_traveled, trip_duration_days, total_receipts_amount\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # === Core Derived Behavior ===\n",
    "    # Per-day spend: in Phase 2, cost_per_day already behaves like spend_per_day\n",
    "    df[\"spend_per_day\"] = df[\"cost_per_day\"]\n",
    "\n",
    "    # Miles per day with safe denominator\n",
    "    df[\"miles_per_day_safe\"] = (\n",
    "        df[\"miles_traveled\"] / df[\"trip_duration_days\"].clip(lower=1)\n",
    "    )\n",
    "\n",
    "    # Log miles for non-linear mileage curve\n",
    "    df[\"log_miles_traveled\"] = np.log1p(df[\"miles_traveled\"])\n",
    "\n",
    "    # === Receipt cents anomalies (.49 / .99 quirk) ===\n",
    "    df[\"receipt_cents\"] = np.round(\n",
    "        df[\"total_receipts_amount\"] - np.floor(df[\"total_receipts_amount\"]), 2\n",
    "    )\n",
    "    df[\"receipt_is_point_49_or_99\"] = df[\"receipt_cents\"].isin([0.49, 0.99]).astype(int)\n",
    "\n",
    "    # === Spend per day optimal band (75–120) & penalties ===\n",
    "    df[\"spend_per_day_good_band\"] = df[\"spend_per_day\"].between(75, 120).astype(int)\n",
    "    df[\"spend_per_day_low\"] = (df[\"spend_per_day\"] < 50).astype(int)\n",
    "    df[\"spend_per_day_high\"] = (df[\"spend_per_day\"] > 120).astype(int)\n",
    "\n",
    "    # === Receipt non-linearity around ~700 ===\n",
    "    df[\"receipts_near_700\"] = df[\"total_receipts_amount\"].between(600, 800).astype(int)\n",
    "    df[\"receipts_very_low\"] = (df[\"total_receipts_amount\"] < 50).astype(int)\n",
    "    df[\"receipts_very_high\"] = (df[\"total_receipts_amount\"] > 1000).astype(int)\n",
    "\n",
    "    # === Efficiency Sweet Spot (4–6 days ∧ 180–220 mi/day) ===\n",
    "    duration_mask = df[\"trip_duration_days\"].between(4, 6)\n",
    "    miles_mask = df[\"miles_per_day_safe\"].between(180, 220)\n",
    "\n",
    "    df[\"is_efficiency_sweet_spot\"] = (duration_mask & miles_mask).astype(int)\n",
    "    df[\"efficiency_score\"] = (\n",
    "        df[\"miles_per_day_safe\"] * df[\"is_efficiency_sweet_spot\"]\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Load Phase 2 dataset + add PRD features\n",
    "# ----------------------------------------------------------\n",
    "combined_df = pd.read_csv(\"phase2_features_baseline_models.csv\")\n",
    "\n",
    "print(\"Dataset loaded for Phase 3!\")\n",
    "print(\"Shape:\", combined_df.shape)\n",
    "\n",
    "combined_df = add_prd_logic_features(combined_df)\n",
    "print(\"PRD-driven logic features added. New shape:\", combined_df.shape)\n",
    "\n",
    "# ---------------------------------------\n",
    "# Feature Selection (ONLY original + PRD)\n",
    "# ---------------------------------------\n",
    "features = [\n",
    "    # Phase 2 continuous engineered inputs\n",
    "    \"trip_duration_days\",\n",
    "    \"miles_traveled\",\n",
    "    \"total_receipts_amount\",\n",
    "    \"cost_per_day\",\n",
    "    \"cost_per_mile\",\n",
    "    \"miles_per_day\",\n",
    "    \"cost_ratio\",\n",
    "\n",
    "    # PRD logic features\n",
    "    \"spend_per_day\",\n",
    "    \"miles_per_day_safe\",\n",
    "    \"log_miles_traveled\",\n",
    "    \"receipt_cents\",\n",
    "    \"receipt_is_point_49_or_99\",\n",
    "    \"spend_per_day_good_band\",\n",
    "    \"spend_per_day_low\",\n",
    "    \"spend_per_day_high\",\n",
    "    \"receipts_near_700\",\n",
    "    \"receipts_very_low\",\n",
    "    \"receipts_very_high\",\n",
    "    \"efficiency_score\",\n",
    "    \"is_efficiency_sweet_spot\",\n",
    "]\n",
    "\n",
    "target = \"reimbursement\"\n",
    "\n",
    "X = combined_df[features]\n",
    "y = combined_df[target]\n",
    "# ==========================================================\n",
    "# Sanity Checks + Random (Shuffled) Train/Test Split\n",
    "# ==========================================================\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"\\n=====  Sanity Checks =====\")\n",
    "\n",
    "# 1) How quantized is reimbursement? (lots of repeated exact values suggests discrete payout bands)\n",
    "y_rounded = combined_df[target].round(2)\n",
    "n_unique = y_rounded.nunique()\n",
    "top_counts = y_rounded.value_counts().head(15)\n",
    "\n",
    "print(f\"Unique reimbursements (rounded to cents): {n_unique}\")\n",
    "print(\"Most common payouts (top 15):\")\n",
    "print(top_counts.to_string())\n",
    "\n",
    "# 2) Do identical feature rows map to multiple reimbursements? (hidden inputs/noise)\n",
    "tmp = combined_df[features + [target]].copy()\n",
    "tmp[target] = tmp[target].round(2)  # compare at cent-level\n",
    "\n",
    "# group by X and count unique y values per identical X row\n",
    "y_variation = tmp.groupby(features)[target].nunique()\n",
    "multi_y_count = int((y_variation > 1).sum())\n",
    "max_y_for_same_x = int(y_variation.max())\n",
    "\n",
    "print(f\"Identical X mapping to >1 reimbursement (cent-rounded): {multi_y_count}\")\n",
    "print(f\"Max distinct reimbursements for the same X: {max_y_for_same_x}\")\n",
    "\n",
    "if multi_y_count > 0:\n",
    "    print(\"  Warning: Same inputs sometimes produce different outputs.\")\n",
    "    print(\"    This suggests hidden variables and/or stochastic behavior, which limits match-rate ceiling.\")\n",
    "\n",
    "# ==========================================================\n",
    "# Random / Shuffled split (recommended vs first-75% / last-25%)\n",
    "# ==========================================================\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.25,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(\"\\n=====  Random Split Applied =====\")\n",
    "print(f\"Training Samples: {len(X_train)}\")\n",
    "print(f\"Testing Samples:  {len(X_test)}\")\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Define Base & Stacking\n",
    "# ----------------------------------------------------------\n",
    "base_models = [\n",
    "    (\"decision_tree\", DecisionTreeRegressor(random_state=42)),\n",
    "    (\"random_forest\", RandomForestRegressor(n_estimators=200, random_state=42)),\n",
    "    (\"gradient_boosting\", GradientBoostingRegressor(random_state=42)),\n",
    "]\n",
    "\n",
    "meta_model = LinearRegression()\n",
    "\n",
    "stack_model = StackingRegressor(\n",
    "    estimators=base_models,\n",
    "    final_estimator=meta_model,\n",
    "    passthrough=True,\n",
    ")\n",
    "\n",
    "stack_model.fit(X_train, y_train)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Evaluate Base Stacked Model\n",
    "# ----------------------------------------------------------\n",
    "stack_pred = stack_model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, stack_pred)\n",
    "rmse = sqrt(mean_squared_error(y_test, stack_pred))\n",
    "r2 = r2_score(y_test, stack_pred)\n",
    "\n",
    "print(\"\\n Ensemble Model Performance (Stacking Regressor):\")\n",
    "print(f\"MAE:  {mae:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"R²:   {r2:.4f}\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Prediction Match Test helper\n",
    "# ----------------------------------------------------------\n",
    "def prediction_match_report(y_true, y_pred, label=\"Phase 3 Prediction Match Test\"):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    abs_diff = np.abs(y_pred - y_true)\n",
    "    print(f\"\\n=====  {label} =====\")\n",
    "    print(f\"Exact Match Rate (<= $0.01): {(abs_diff <= 0.01).mean():.4f}\")\n",
    "    print(f\"Close Match Rate (<= $1.00): {(abs_diff <= 1.00).mean():.4f}\")\n",
    "    print(f\"±$5 Accuracy: {(abs_diff <= 5.00).mean():.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\n=====  Base Ensemble Match Rates (Test) =====\")\n",
    "prediction_match_report(y_test, stack_pred, label=\"Base Ensemble Match Rates (Test)\")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Residual Calibration\n",
    "# ----------------------------------------------------------\n",
    "from sklearn.ensemble import GradientBoostingRegressor as GBRResidual\n",
    "\n",
    "train_base_pred = stack_model.predict(X_train)\n",
    "train_residuals = y_train - train_base_pred\n",
    "\n",
    "residual_model = GBRResidual(\n",
    "    random_state=42,\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=2,\n",
    "    min_samples_leaf=20,\n",
    ")\n",
    "\n",
    "residual_model.fit(X_train, train_residuals)\n",
    "\n",
    "print(\"\\n Residual correction model trained!\")\n",
    "\n",
    "# Corrections\n",
    "train_corrections = residual_model.predict(X_train)\n",
    "test_corrections = residual_model.predict(X_test)\n",
    "\n",
    "# Calibrated predictions\n",
    "train_final_pred = train_base_pred + train_corrections\n",
    "test_final_pred = stack_pred + test_corrections\n",
    "\n",
    "# Calibrated metrics\n",
    "calib_mae = mean_absolute_error(y_test, test_final_pred)\n",
    "calib_rmse = sqrt(mean_squared_error(y_test, test_final_pred))\n",
    "calib_r2 = r2_score(y_test, test_final_pred)\n",
    "\n",
    "print(\"\\n  Calibrated Ensemble Performance (with Residual Model):\")\n",
    "print(f\"MAE:  {calib_mae:.4f}\")\n",
    "print(f\"RMSE: {calib_rmse:.4f}\")\n",
    "print(f\"R²:   {calib_r2:.4f}\")\n",
    "\n",
    "print(\"\\n=====  Calibrated Ensemble Match Rates (Test – Raw) =====\")\n",
    "prediction_match_report(y_test, test_final_pred, label=\"Calibrated Ensemble Match Rates (Test – Raw)\")\n",
    "\n",
    "# Optional: rounded to cents\n",
    "test_final_pred_rounded = np.round(test_final_pred, 2)\n",
    "print(\"\\n=====  Calibrated Ensemble Match Rates (Test – Rounded to Cents) =====\")\n",
    "prediction_match_report(\n",
    "    y_test, test_final_pred_rounded, label=\"Calibrated Ensemble Match Rates (Test – Rounded to Cents)\"\n",
    ")\n",
    "\n",
    "print(\"\\n=====  Calibrated Ensemble Match Rates (Train) =====\")\n",
    "prediction_match_report(y_train, train_final_pred, label=\"Calibrated Ensemble Match Rates (Train)\")\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# Advanced Techniques – SVR, MLP, Voting Ensemble\n",
    "# ==========================================================\n",
    "\n",
    "# --- Support Vector Regression (SVR) ---\n",
    "svr_model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    SVR(kernel=\"rbf\", C=10.0, epsilon=5.0),\n",
    ")\n",
    "\n",
    "svr_model.fit(X_train, y_train)\n",
    "svr_pred = svr_model.predict(X_test)\n",
    "\n",
    "svr_mae = mean_absolute_error(y_test, svr_pred)\n",
    "svr_rmse = sqrt(mean_squared_error(y_test, svr_pred))\n",
    "svr_r2 = r2_score(y_test, svr_pred)\n",
    "\n",
    "print(\"\\n SVR Performance:\")\n",
    "print(f\"MAE:  {svr_mae:.4f}\")\n",
    "print(f\"RMSE: {svr_rmse:.4f}\")\n",
    "print(f\"R²:   {svr_r2:.4f}\")\n",
    "prediction_match_report(y_test, svr_pred, label=\"SVR Match Rates (Test)\")\n",
    "\n",
    "\n",
    "# --- Neural Network (MLPRegressor) ---\n",
    "mlp_model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    MLPRegressor(\n",
    "        hidden_layer_sizes=(64, 32),\n",
    "        activation=\"relu\",\n",
    "        solver=\"adam\",\n",
    "        alpha=1e-3,\n",
    "        max_iter=2000,\n",
    "        random_state=42,\n",
    "    ),\n",
    ")\n",
    "\n",
    "mlp_model.fit(X_train, y_train)\n",
    "mlp_pred = mlp_model.predict(X_test)\n",
    "\n",
    "mlp_mae = mean_absolute_error(y_test, mlp_pred)\n",
    "mlp_rmse = sqrt(mean_squared_error(y_test, mlp_pred))\n",
    "mlp_r2 = r2_score(y_test, mlp_pred)\n",
    "\n",
    "print(\"\\n MLP Performance:\")\n",
    "print(f\"MAE:  {mlp_mae:.4f}\")\n",
    "print(f\"RMSE: {mlp_rmse:.4f}\")\n",
    "print(f\"R²:   {mlp_r2:.4f}\")\n",
    "prediction_match_report(y_test, mlp_pred, label=\"MLP Match Rates (Test)\")\n",
    "\n",
    "\n",
    "# --- Voting Regressor (simple averaging ensemble) ---\n",
    "voting_reg = VotingRegressor(\n",
    "    estimators=[\n",
    "        (\"rf\", base_models[1][1]),   # RandomForestRegressor\n",
    "        (\"gbr\", base_models[2][1]),  # GradientBoostingRegressor\n",
    "        (\"tree\", base_models[0][1]), # DecisionTreeRegressor\n",
    "    ]\n",
    ")\n",
    "\n",
    "voting_reg.fit(X_train, y_train)\n",
    "voting_pred = voting_reg.predict(X_test)\n",
    "\n",
    "v_mae = mean_absolute_error(y_test, voting_pred)\n",
    "v_rmse = sqrt(mean_squared_error(y_test, voting_pred))\n",
    "v_r2 = r2_score(y_test, voting_pred)\n",
    "\n",
    "print(\"\\n Voting Regressor Performance:\")\n",
    "print(f\"MAE:  {v_mae:.4f}\")\n",
    "print(f\"RMSE: {v_rmse:.4f}\")\n",
    "print(f\"R²:   {v_r2:.4f}\")\n",
    "prediction_match_report(y_test, voting_pred, label=\"Voting Regressor Match Rates (Test)\")\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# Rule-Based Learning – Surrogate Tree for Interpretability\n",
    "# ==========================================================\n",
    "\n",
    "# Train a shallow tree to mimic the calibrated ensemble behavior\n",
    "surrogate_tree = DecisionTreeRegressor(\n",
    "    max_depth=4,\n",
    "    min_samples_leaf=20,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Use calibrated train predictions as the \"target\" for the surrogate\n",
    "surrogate_tree.fit(X_train, train_final_pred)\n",
    "\n",
    "tree_rules = export_text(surrogate_tree, feature_names=list(X.columns))\n",
    "\n",
    "print(\"\\n=== Surrogate Tree Rules (approximate learned logic) ===\")\n",
    "print(tree_rules)\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# DISCRETE / TIERED MODELING – Classification over Payout Buckets\n",
    "# (FIXED for random train_test_split indexing)\n",
    "# ==========================================================\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "bucket_size = 5.0  # try 5.0, 2.0, 1.0 (but 1.0 creates many classes)\n",
    "\n",
    "# Build bucket labels from y (NOT from sequential slicing)\n",
    "y_bucket_all = (y.round(2) / bucket_size).round().astype(int)\n",
    "\n",
    "# Align buckets with the randomized split via indices\n",
    "y_bucket_train = y_bucket_all.loc[X_train.index]\n",
    "y_bucket_test  = y_bucket_all.loc[X_test.index]\n",
    "\n",
    "# Map: bucket -> representative payout (median payout in TRAIN for that bucket)\n",
    "train_bucket_df = pd.DataFrame({\n",
    "    \"bucket\": y_bucket_train,\n",
    "    \"payout\": y_train\n",
    "})\n",
    "bucket_to_median = train_bucket_df.groupby(\"bucket\")[\"payout\"].median().to_dict()\n",
    "\n",
    "def buckets_to_payouts(bucket_array):\n",
    "    bucket_array = np.asarray(bucket_array)\n",
    "    return np.array([bucket_to_median.get(int(b), float(b) * bucket_size) for b in bucket_array], dtype=float)\n",
    "\n",
    "rf_cls = RandomForestClassifier(\n",
    "    n_estimators=400,\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_cls.fit(X_train, y_bucket_train)\n",
    "\n",
    "bucket_pred_test = rf_cls.predict(X_test)\n",
    "tier_pred_test = buckets_to_payouts(bucket_pred_test)\n",
    "\n",
    "print(\"\\n Tier Classifier (RF over buckets) Performance:\")\n",
    "print(f\"MAE:  {mean_absolute_error(y_test, tier_pred_test):.4f}\")\n",
    "print(f\"RMSE: {sqrt(mean_squared_error(y_test, tier_pred_test)):.4f}\")\n",
    "print(f\"R²:   {r2_score(y_test, tier_pred_test):.4f}\")\n",
    "prediction_match_report(y_test, tier_pred_test, label=\"Tier Classifier Match Rates (Test)\")\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# CASE-BASED REASONING – kNN Regressor (IMPORTANT: scaling + distance weights)\n",
    "# ==========================================================\n",
    "\n",
    "# Your previous NearestNeighbors version used raw feature scales\n",
    "# => distances dominated by receipts/miles. Scale helps a lot for neighbor quality.\n",
    "\n",
    "knn_reg = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    KNeighborsRegressor(\n",
    "        n_neighbors=3,         # try 1,3,5,7\n",
    "        weights=\"distance\",    # closer neighbors matter more\n",
    "        metric=\"minkowski\",\n",
    "        p=2\n",
    "    )\n",
    ")\n",
    "\n",
    "knn_reg.fit(X_train, y_train)\n",
    "knn_pred = knn_reg.predict(X_test)\n",
    "\n",
    "print(\"\\n kNN Regressor Performance (scaled, distance-weighted):\")\n",
    "print(f\"MAE:  {mean_absolute_error(y_test, knn_pred):.4f}\")\n",
    "print(f\"RMSE: {sqrt(mean_squared_error(y_test, knn_pred)):.4f}\")\n",
    "print(f\"R²:   {r2_score(y_test, knn_pred):.4f}\")\n",
    "prediction_match_report(y_test, knn_pred, label=\"kNN Regressor Match Rates (Test)\")\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "a6bc346c9a05e226"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Directly optimize close match rate\n",
    "\n",
    "\n"
   ],
   "id": "8ff074d0ad9be3ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ==========================================================\n",
    "# Phase 3 – Ensemble Learning (Stacking Regressor)\n",
    "# With ONLY Original + PRD Logic Features (NO rule flags)\n",
    "# ==========================================================\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import (\n",
    "    StackingRegressor,\n",
    "    RandomForestRegressor,\n",
    "    GradientBoostingRegressor,\n",
    "    VotingRegressor,\n",
    "    RandomForestClassifier,\n",
    ")\n",
    "from sklearn.tree import DecisionTreeRegressor, export_text\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# ---------------------------------------\n",
    "# PRD-driven feature engineering\n",
    "# ---------------------------------------\n",
    "\n",
    "def add_prd_logic_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds PRD-derived behavioral features:\n",
    "      - spend_per_day curved behavior\n",
    "      - efficiency reward band (4–6 days & 180–220 mi/day)\n",
    "      - log miles decay\n",
    "      - receipt cents anomalies (.49 / .99)\n",
    "      - receipts curve near $700 (bonus)\n",
    "      - extreme receipt conditions\n",
    "\n",
    "    Assumes Phase 2 features already exist:\n",
    "        cost_per_day, miles_traveled, trip_duration_days, total_receipts_amount\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # === Core Derived Behavior ===\n",
    "    # Per-day spend: in Phase 2, cost_per_day already behaves like spend_per_day\n",
    "    df[\"spend_per_day\"] = df[\"cost_per_day\"]\n",
    "\n",
    "    # Miles per day with safe denominator\n",
    "    df[\"miles_per_day_safe\"] = (\n",
    "        df[\"miles_traveled\"] / df[\"trip_duration_days\"].clip(lower=1)\n",
    "    )\n",
    "\n",
    "    # Log miles for non-linear mileage curve\n",
    "    df[\"log_miles_traveled\"] = np.log1p(df[\"miles_traveled\"])\n",
    "\n",
    "    # === Receipt cents anomalies (.49 / .99 quirk) ===\n",
    "    df[\"receipt_cents\"] = np.round(\n",
    "        df[\"total_receipts_amount\"] - np.floor(df[\"total_receipts_amount\"]), 2\n",
    "    )\n",
    "    df[\"receipt_is_point_49_or_99\"] = df[\"receipt_cents\"].isin([0.49, 0.99]).astype(int)\n",
    "\n",
    "    # === Spend per day optimal band (75–120) & penalties ===\n",
    "    df[\"spend_per_day_good_band\"] = df[\"spend_per_day\"].between(75, 120).astype(int)\n",
    "    df[\"spend_per_day_low\"] = (df[\"spend_per_day\"] < 50).astype(int)\n",
    "    df[\"spend_per_day_high\"] = (df[\"spend_per_day\"] > 120).astype(int)\n",
    "\n",
    "    # === Receipt non-linearity around ~700 ===\n",
    "    df[\"receipts_near_700\"] = df[\"total_receipts_amount\"].between(600, 800).astype(int)\n",
    "    df[\"receipts_very_low\"] = (df[\"total_receipts_amount\"] < 50).astype(int)\n",
    "    df[\"receipts_very_high\"] = (df[\"total_receipts_amount\"] > 1000).astype(int)\n",
    "\n",
    "    # === Efficiency Sweet Spot (4–6 days ∧ 180–220 mi/day) ===\n",
    "    duration_mask = df[\"trip_duration_days\"].between(4, 6)\n",
    "    miles_mask = df[\"miles_per_day_safe\"].between(180, 220)\n",
    "\n",
    "    df[\"is_efficiency_sweet_spot\"] = (duration_mask & miles_mask).astype(int)\n",
    "    df[\"efficiency_score\"] = (\n",
    "        df[\"miles_per_day_safe\"] * df[\"is_efficiency_sweet_spot\"]\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Load Phase 2 dataset + add PRD features\n",
    "# ----------------------------------------------------------\n",
    "combined_df = pd.read_csv(\"phase2_features_baseline_models.csv\")\n",
    "\n",
    "print(\"Dataset loaded for Phase 3!\")\n",
    "print(\"Shape:\", combined_df.shape)\n",
    "\n",
    "combined_df = add_prd_logic_features(combined_df)\n",
    "print(\"PRD-driven logic features added. New shape:\", combined_df.shape)\n",
    "\n",
    "# ---------------------------------------\n",
    "# Feature Selection (ONLY original + PRD)\n",
    "# ---------------------------------------\n",
    "features = [\n",
    "    # Phase 2 continuous engineered inputs\n",
    "    \"trip_duration_days\",\n",
    "    \"miles_traveled\",\n",
    "    \"total_receipts_amount\",\n",
    "    \"cost_per_day\",\n",
    "    \"cost_per_mile\",\n",
    "    \"miles_per_day\",\n",
    "    \"cost_ratio\",\n",
    "\n",
    "    # PRD logic features\n",
    "    \"spend_per_day\",\n",
    "    \"miles_per_day_safe\",\n",
    "    \"log_miles_traveled\",\n",
    "    \"receipt_cents\",\n",
    "    \"receipt_is_point_49_or_99\",\n",
    "    \"spend_per_day_good_band\",\n",
    "    \"spend_per_day_low\",\n",
    "    \"spend_per_day_high\",\n",
    "    \"receipts_near_700\",\n",
    "    \"receipts_very_low\",\n",
    "    \"receipts_very_high\",\n",
    "    \"efficiency_score\",\n",
    "    \"is_efficiency_sweet_spot\",\n",
    "]\n",
    "\n",
    "target = \"reimbursement\"\n",
    "\n",
    "X = combined_df[features]\n",
    "y = combined_df[target]\n",
    "# ==========================================================\n",
    "# Sanity Checks + Random (Shuffled) Train/Test Split\n",
    "# ==========================================================\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"\\n===== 🔍 Sanity Checks =====\")\n",
    "\n",
    "# 1) How quantized is reimbursement? (lots of repeated exact values suggests discrete payout bands)\n",
    "y_rounded = combined_df[target].round(2)\n",
    "n_unique = y_rounded.nunique()\n",
    "top_counts = y_rounded.value_counts().head(15)\n",
    "\n",
    "print(f\"Unique reimbursements (rounded to cents): {n_unique}\")\n",
    "print(\"Most common payouts (top 15):\")\n",
    "print(top_counts.to_string())\n",
    "\n",
    "# 2) Do identical feature rows map to multiple reimbursements? (hidden inputs/noise)\n",
    "tmp = combined_df[features + [target]].copy()\n",
    "tmp[target] = tmp[target].round(2)  # compare at cent-level\n",
    "\n",
    "# group by X and count unique y values per identical X row\n",
    "y_variation = tmp.groupby(features)[target].nunique()\n",
    "multi_y_count = int((y_variation > 1).sum())\n",
    "max_y_for_same_x = int(y_variation.max())\n",
    "\n",
    "print(f\"Identical X mapping to >1 reimbursement (cent-rounded): {multi_y_count}\")\n",
    "print(f\"Max distinct reimbursements for the same X: {max_y_for_same_x}\")\n",
    "\n",
    "if multi_y_count > 0:\n",
    "    print(\"⚠️  Warning: Same inputs sometimes produce different outputs.\")\n",
    "    print(\"    This suggests hidden variables and/or stochastic behavior, which limits match-rate ceiling.\")\n",
    "\n",
    "# ==========================================================\n",
    "# Random / Shuffled split (recommended vs first-75% / last-25%)\n",
    "# ==========================================================\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.25,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(\"\\n===== ✅ Random Split Applied =====\")\n",
    "print(f\"Training Samples: {len(X_train)}\")\n",
    "print(f\"Testing Samples:  {len(X_test)}\")\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Define Base & Stacking\n",
    "# ----------------------------------------------------------\n",
    "base_models = [\n",
    "    (\"decision_tree\", DecisionTreeRegressor(random_state=42)),\n",
    "    (\"random_forest\", RandomForestRegressor(n_estimators=200, random_state=42)),\n",
    "    (\"gradient_boosting\", GradientBoostingRegressor(random_state=42)),\n",
    "]\n",
    "\n",
    "meta_model = LinearRegression()\n",
    "\n",
    "stack_model = StackingRegressor(\n",
    "    estimators=base_models,\n",
    "    final_estimator=meta_model,\n",
    "    passthrough=True,\n",
    ")\n",
    "\n",
    "stack_model.fit(X_train, y_train)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Evaluate Base Stacked Model\n",
    "# ----------------------------------------------------------\n",
    "stack_pred = stack_model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, stack_pred)\n",
    "rmse = sqrt(mean_squared_error(y_test, stack_pred))\n",
    "r2 = r2_score(y_test, stack_pred)\n",
    "\n",
    "print(\"\\n Ensemble Model Performance (Stacking Regressor):\")\n",
    "print(f\"MAE:  {mae:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"R²:   {r2:.4f}\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Prediction Match Test helper\n",
    "# ----------------------------------------------------------\n",
    "def prediction_match_report(y_true, y_pred, label=\"Phase 3 Prediction Match Test\"):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    abs_diff = np.abs(y_pred - y_true)\n",
    "    print(f\"\\n=====  {label} =====\")\n",
    "    print(f\"Exact Match Rate (<= $0.01): {(abs_diff <= 0.01).mean():.4f}\")\n",
    "    print(f\"Close Match Rate (<= $1.00): {(abs_diff <= 1.00).mean():.4f}\")\n",
    "    print(f\"±$5 Accuracy: {(abs_diff <= 5.00).mean():.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\n=====  Base Ensemble Match Rates (Test) =====\")\n",
    "prediction_match_report(y_test, stack_pred, label=\"Base Ensemble Match Rates (Test)\")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Residual Calibration\n",
    "# ----------------------------------------------------------\n",
    "from sklearn.ensemble import GradientBoostingRegressor as GBRResidual\n",
    "\n",
    "train_base_pred = stack_model.predict(X_train)\n",
    "train_residuals = y_train - train_base_pred\n",
    "\n",
    "residual_model = GBRResidual(\n",
    "    random_state=42,\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=2,\n",
    "    min_samples_leaf=20,\n",
    ")\n",
    "\n",
    "residual_model.fit(X_train, train_residuals)\n",
    "\n",
    "print(\"\\n Residual correction model trained!\")\n",
    "\n",
    "# Corrections\n",
    "train_corrections = residual_model.predict(X_train)\n",
    "test_corrections = residual_model.predict(X_test)\n",
    "\n",
    "# Calibrated predictions\n",
    "train_final_pred = train_base_pred + train_corrections\n",
    "test_final_pred = stack_pred + test_corrections\n",
    "\n",
    "# Calibrated metrics\n",
    "calib_mae = mean_absolute_error(y_test, test_final_pred)\n",
    "calib_rmse = sqrt(mean_squared_error(y_test, test_final_pred))\n",
    "calib_r2 = r2_score(y_test, test_final_pred)\n",
    "\n",
    "print(\"\\n  Calibrated Ensemble Performance (with Residual Model):\")\n",
    "print(f\"MAE:  {calib_mae:.4f}\")\n",
    "print(f\"RMSE: {calib_rmse:.4f}\")\n",
    "print(f\"R²:   {calib_r2:.4f}\")\n",
    "\n",
    "print(\"\\n=====  Calibrated Ensemble Match Rates (Test – Raw) =====\")\n",
    "prediction_match_report(y_test, test_final_pred, label=\"Calibrated Ensemble Match Rates (Test – Raw)\")\n",
    "\n",
    "# Optional: rounded to cents\n",
    "test_final_pred_rounded = np.round(test_final_pred, 2)\n",
    "print(\"\\n=====  Calibrated Ensemble Match Rates (Test – Rounded to Cents) =====\")\n",
    "prediction_match_report(\n",
    "    y_test, test_final_pred_rounded, label=\"Calibrated Ensemble Match Rates (Test – Rounded to Cents)\"\n",
    ")\n",
    "\n",
    "print(\"\\n=====  Calibrated Ensemble Match Rates (Train) =====\")\n",
    "prediction_match_report(y_train, train_final_pred, label=\"Calibrated Ensemble Match Rates (Train)\")\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# Advanced Techniques – SVR, MLP, Voting Ensemble\n",
    "# ==========================================================\n",
    "\n",
    "# --- Support Vector Regression (SVR) ---\n",
    "svr_model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    SVR(kernel=\"rbf\", C=10.0, epsilon=5.0),\n",
    ")\n",
    "\n",
    "svr_model.fit(X_train, y_train)\n",
    "svr_pred = svr_model.predict(X_test)\n",
    "\n",
    "svr_mae = mean_absolute_error(y_test, svr_pred)\n",
    "svr_rmse = sqrt(mean_squared_error(y_test, svr_pred))\n",
    "svr_r2 = r2_score(y_test, svr_pred)\n",
    "\n",
    "print(\"\\n SVR Performance:\")\n",
    "print(f\"MAE:  {svr_mae:.4f}\")\n",
    "print(f\"RMSE: {svr_rmse:.4f}\")\n",
    "print(f\"R²:   {svr_r2:.4f}\")\n",
    "prediction_match_report(y_test, svr_pred, label=\"SVR Match Rates (Test)\")\n",
    "\n",
    "\n",
    "# --- Neural Network (MLPRegressor) ---\n",
    "mlp_model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    MLPRegressor(\n",
    "        hidden_layer_sizes=(64, 32),\n",
    "        activation=\"relu\",\n",
    "        solver=\"adam\",\n",
    "        alpha=1e-3,\n",
    "        max_iter=2000,\n",
    "        random_state=42,\n",
    "    ),\n",
    ")\n",
    "\n",
    "mlp_model.fit(X_train, y_train)\n",
    "mlp_pred = mlp_model.predict(X_test)\n",
    "\n",
    "mlp_mae = mean_absolute_error(y_test, mlp_pred)\n",
    "mlp_rmse = sqrt(mean_squared_error(y_test, mlp_pred))\n",
    "mlp_r2 = r2_score(y_test, mlp_pred)\n",
    "\n",
    "print(\"\\n MLP Performance:\")\n",
    "print(f\"MAE:  {mlp_mae:.4f}\")\n",
    "print(f\"RMSE: {mlp_rmse:.4f}\")\n",
    "print(f\"R²:   {mlp_r2:.4f}\")\n",
    "prediction_match_report(y_test, mlp_pred, label=\"MLP Match Rates (Test)\")\n",
    "\n",
    "\n",
    "# --- Voting Regressor (simple averaging ensemble) ---\n",
    "voting_reg = VotingRegressor(\n",
    "    estimators=[\n",
    "        (\"rf\", base_models[1][1]),   # RandomForestRegressor\n",
    "        (\"gbr\", base_models[2][1]),  # GradientBoostingRegressor\n",
    "        (\"tree\", base_models[0][1]), # DecisionTreeRegressor\n",
    "    ]\n",
    ")\n",
    "\n",
    "voting_reg.fit(X_train, y_train)\n",
    "voting_pred = voting_reg.predict(X_test)\n",
    "\n",
    "v_mae = mean_absolute_error(y_test, voting_pred)\n",
    "v_rmse = sqrt(mean_squared_error(y_test, voting_pred))\n",
    "v_r2 = r2_score(y_test, voting_pred)\n",
    "\n",
    "print(\"\\n Voting Regressor Performance:\")\n",
    "print(f\"MAE:  {v_mae:.4f}\")\n",
    "print(f\"RMSE: {v_rmse:.4f}\")\n",
    "print(f\"R²:   {v_r2:.4f}\")\n",
    "prediction_match_report(y_test, voting_pred, label=\"Voting Regressor Match Rates (Test)\")\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# Rule-Based Learning – Surrogate Tree for Interpretability\n",
    "# ==========================================================\n",
    "\n",
    "# Train a shallow tree to mimic the calibrated ensemble behavior\n",
    "surrogate_tree = DecisionTreeRegressor(\n",
    "    max_depth=4,\n",
    "    min_samples_leaf=20,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Use calibrated train predictions as the \"target\" for the surrogate\n",
    "surrogate_tree.fit(X_train, train_final_pred)\n",
    "\n",
    "tree_rules = export_text(surrogate_tree, feature_names=list(X.columns))\n",
    "\n",
    "print(\"\\n=== Surrogate Tree Rules (approximate learned logic) ===\")\n",
    "print(tree_rules)\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# DISCRETE / TIERED MODELING – Classification over Payout Buckets\n",
    "# (FIXED for random train_test_split indexing)\n",
    "# ==========================================================\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "bucket_size = 5.0  # try 5.0, 2.0, 1.0 (but 1.0 creates many classes)\n",
    "\n",
    "# Build bucket labels from y (NOT from sequential slicing)\n",
    "y_bucket_all = (y.round(2) / bucket_size).round().astype(int)\n",
    "\n",
    "# Align buckets with the randomized split via indices\n",
    "y_bucket_train = y_bucket_all.loc[X_train.index]\n",
    "y_bucket_test  = y_bucket_all.loc[X_test.index]\n",
    "\n",
    "# Map: bucket -> representative payout (median payout in TRAIN for that bucket)\n",
    "train_bucket_df = pd.DataFrame({\n",
    "    \"bucket\": y_bucket_train,\n",
    "    \"payout\": y_train\n",
    "})\n",
    "bucket_to_median = train_bucket_df.groupby(\"bucket\")[\"payout\"].median().to_dict()\n",
    "\n",
    "def buckets_to_payouts(bucket_array):\n",
    "    bucket_array = np.asarray(bucket_array)\n",
    "    return np.array([bucket_to_median.get(int(b), float(b) * bucket_size) for b in bucket_array], dtype=float)\n",
    "\n",
    "rf_cls = RandomForestClassifier(\n",
    "    n_estimators=400,\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_cls.fit(X_train, y_bucket_train)\n",
    "\n",
    "bucket_pred_test = rf_cls.predict(X_test)\n",
    "tier_pred_test = buckets_to_payouts(bucket_pred_test)\n",
    "\n",
    "print(\"\\n Tier Classifier (RF over buckets) Performance:\")\n",
    "print(f\"MAE:  {mean_absolute_error(y_test, tier_pred_test):.4f}\")\n",
    "print(f\"RMSE: {sqrt(mean_squared_error(y_test, tier_pred_test)):.4f}\")\n",
    "print(f\"R²:   {r2_score(y_test, tier_pred_test):.4f}\")\n",
    "prediction_match_report(y_test, tier_pred_test, label=\"Tier Classifier Match Rates (Test)\")\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# CASE-BASED REASONING – kNN Regressor (IMPORTANT: scaling + distance weights)\n",
    "# ==========================================================\n",
    "\n",
    "# Your previous NearestNeighbors version used raw feature scales\n",
    "# => distances dominated by receipts/miles. Scale helps a lot for neighbor quality.\n",
    "\n",
    "knn_reg = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    KNeighborsRegressor(\n",
    "        n_neighbors=3,         # try 1,3,5,7\n",
    "        weights=\"distance\",    # closer neighbors matter more\n",
    "        metric=\"minkowski\",\n",
    "        p=2\n",
    "    )\n",
    ")\n",
    "\n",
    "knn_reg.fit(X_train, y_train)\n",
    "knn_pred = knn_reg.predict(X_test)\n",
    "\n",
    "print(\"\\n kNN Regressor Performance (scaled, distance-weighted):\")\n",
    "print(f\"MAE:  {mean_absolute_error(y_test, knn_pred):.4f}\")\n",
    "print(f\"RMSE: {sqrt(mean_squared_error(y_test, knn_pred)):.4f}\")\n",
    "print(f\"R²:   {r2_score(y_test, knn_pred):.4f}\")\n",
    "prediction_match_report(y_test, knn_pred, label=\"kNN Regressor Match Rates (Test)\")\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "c45c757b68391f6d"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
