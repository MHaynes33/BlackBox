{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T20:56:40.954559Z",
     "start_time": "2025-12-04T20:56:34.504174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==========================================================\n",
    "# Phase 3 ‚Äì Stacking Regressor + Residual Calibration ONLY\n",
    "# + Over/Underfitting Diagnostics (Train vs Test)\n",
    "# + REGULARIZATION to reduce overfitting\n",
    "# ==========================================================\n",
    "\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import StackingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "# PRD-driven feature engineering\n",
    "# ---------------------------------------\n",
    "def add_prd_logic_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    df[\"spend_per_day\"] = df[\"cost_per_day\"]\n",
    "    df[\"miles_per_day_safe\"] = df[\"miles_traveled\"] / df[\"trip_duration_days\"].clip(lower=1)\n",
    "    df[\"log_miles_traveled\"] = np.log1p(df[\"miles_traveled\"])\n",
    "\n",
    "    df[\"receipt_cents\"] = np.round(df[\"total_receipts_amount\"] - np.floor(df[\"total_receipts_amount\"]), 2)\n",
    "    df[\"receipt_is_point_49_or_99\"] = df[\"receipt_cents\"].isin([0.49, 0.99]).astype(int)\n",
    "\n",
    "    df[\"spend_per_day_good_band\"] = df[\"spend_per_day\"].between(75, 120).astype(int)\n",
    "    df[\"spend_per_day_low\"] = (df[\"spend_per_day\"] < 50).astype(int)\n",
    "    df[\"spend_per_day_high\"] = (df[\"spend_per_day\"] > 120).astype(int)\n",
    "\n",
    "    df[\"receipts_near_700\"] = df[\"total_receipts_amount\"].between(600, 800).astype(int)\n",
    "    df[\"receipts_very_low\"] = (df[\"total_receipts_amount\"] < 50).astype(int)\n",
    "    df[\"receipts_very_high\"] = (df[\"total_receipts_amount\"] > 1000).astype(int)\n",
    "\n",
    "    duration_mask = df[\"trip_duration_days\"].between(4, 6)\n",
    "    miles_mask = df[\"miles_per_day_safe\"].between(180, 220)\n",
    "    df[\"is_efficiency_sweet_spot\"] = (duration_mask & miles_mask).astype(int)\n",
    "    df[\"efficiency_score\"] = df[\"miles_per_day_safe\"] * df[\"is_efficiency_sweet_spot\"]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "# Match-rate helper\n",
    "# ---------------------------------------\n",
    "def prediction_match_report(y_true, y_pred, label):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    abs_diff = np.abs(y_pred - y_true)\n",
    "\n",
    "    print(f\"\\n=====  {label} =====\")\n",
    "    print(f\"Exact Match Rate (<= $0.01): {(abs_diff <= 0.01).mean():.4f}\")\n",
    "    print(f\"Close Match Rate (<= $1.00): {(abs_diff <= 1.00).mean():.4f}\")\n",
    "    print(f\"¬±$5 Accuracy: {(abs_diff <= 5.00).mean():.4f}\")\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "# Over/Underfitting diagnostics helper\n",
    "# ---------------------------------------\n",
    "def fit_diagnostics(y_train_true, y_train_pred, y_test_true, y_test_pred, label=\"Model\"):\n",
    "    def _metrics(y_true, y_pred):\n",
    "        y_true = np.asarray(y_true, dtype=float)\n",
    "        y_pred = np.asarray(y_pred, dtype=float)\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        rmse = sqrt(mean_squared_error(y_true, y_pred))\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        abs_diff = np.abs(y_pred - y_true)\n",
    "        within1 = (abs_diff <= 1.0).mean()\n",
    "        within5 = (abs_diff <= 5.0).mean()\n",
    "        return mae, rmse, r2, within1, within5\n",
    "\n",
    "    tr = _metrics(y_train_true, y_train_pred)\n",
    "    te = _metrics(y_test_true, y_test_pred)\n",
    "\n",
    "    print(f\"\\n=====  Fit Diagnostics: {label} =====\")\n",
    "    print(\"Train -> MAE: {:.2f} | RMSE: {:.2f} | R¬≤: {:.4f} | ‚â§$1: {:.3f} | ‚â§$5: {:.3f}\".format(*tr))\n",
    "    print(\"Test  -> MAE: {:.2f} | RMSE: {:.2f} | R¬≤: {:.4f} | ‚â§$1: {:.3f} | ‚â§$5: {:.3f}\".format(*te))\n",
    "\n",
    "    print(\"\\n----- Generalization Gaps (Train - Test) -----\")\n",
    "    print(f\"MAE gap:  {tr[0] - te[0]:.2f}\")\n",
    "    print(f\"RMSE gap: {tr[1] - te[1]:.2f}\")\n",
    "    print(f\"R¬≤ gap:   {tr[2] - te[2]:.4f}\")\n",
    "    print(f\"‚â§$1 gap:  {tr[3] - te[3]:.3f}\")\n",
    "    print(f\"‚â§$5 gap:  {tr[4] - te[4]:.3f}\")\n",
    "\n",
    "    if (tr[0] < te[0] - 20) or (tr[3] > te[3] + 0.10):\n",
    "        print(\"\\n Likely OVERFITTING: train is much better than test.\")\n",
    "    elif (tr[3] < 0.05 and te[3] < 0.05) and (tr[0] > 30 and te[0] > 30):\n",
    "        print(\"\\n Likely UNDERFITTING *for match-rate*: both train/test ‚â§$1 are low.\")\n",
    "    else:\n",
    "        print(\"\\n No strong overfitting signal from these thresholds.\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Load + feature engineer\n",
    "# ----------------------------------------------------------\n",
    "combined_df = pd.read_csv(\"phase2_features_baseline_models.csv\")\n",
    "print(\"Dataset loaded for Phase 3!\")\n",
    "print(\"Shape:\", combined_df.shape)\n",
    "\n",
    "combined_df = add_prd_logic_features(combined_df)\n",
    "print(\"PRD-driven logic features added. New shape:\", combined_df.shape)\n",
    "\n",
    "features = [\n",
    "    \"trip_duration_days\", \"miles_traveled\", \"total_receipts_amount\",\n",
    "    \"cost_per_day\", \"cost_per_mile\", \"miles_per_day\", \"cost_ratio\",\n",
    "    \"spend_per_day\", \"miles_per_day_safe\", \"log_miles_traveled\",\n",
    "    \"receipt_cents\", \"receipt_is_point_49_or_99\",\n",
    "    \"spend_per_day_good_band\", \"spend_per_day_low\", \"spend_per_day_high\",\n",
    "    \"receipts_near_700\", \"receipts_very_low\", \"receipts_very_high\",\n",
    "    \"efficiency_score\", \"is_efficiency_sweet_spot\",\n",
    "]\n",
    "target = \"reimbursement\"\n",
    "\n",
    "missing = [c for c in (features + [target]) if c not in combined_df.columns]\n",
    "if missing:\n",
    "    raise KeyError(f\"Missing required columns in dataset: {missing}\")\n",
    "\n",
    "X = combined_df[features].copy()\n",
    "y = combined_df[target].copy()\n",
    "\n",
    "X = X.replace([np.inf, -np.inf], np.nan)\n",
    "if X.isna().any().any():\n",
    "    X = X.fillna(X.median(numeric_only=True))\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Split\n",
    "# ----------------------------------------------------------\n",
    "USE_SHUFFLE_SPLIT = True\n",
    "\n",
    "if USE_SHUFFLE_SPLIT:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.25, random_state=42, shuffle=True\n",
    "    )\n",
    "    print(\"\\n=====  Random Split Applied =====\")\n",
    "else:\n",
    "    split = int(0.75 * len(X))\n",
    "    X_train, X_test = X.iloc[:split], X.iloc[split:]\n",
    "    y_train, y_test = y.iloc[:split], y.iloc[split:]\n",
    "    print(\"\\n=====  Sequential Split Applied =====\")\n",
    "\n",
    "print(f\"Training Samples: {len(X_train)}\")\n",
    "print(f\"Testing Samples:  {len(X_test)}\")\n",
    "\n",
    "# ==========================================================\n",
    "# REGULARIZED Stacking Regressor (reduce overfitting)\n",
    "# ==========================================================\n",
    "base_models = [\n",
    "    # Regularize the tree (stop perfect memorization)\n",
    "    (\"decision_tree\", DecisionTreeRegressor(\n",
    "        random_state=42,\n",
    "        max_depth=6,\n",
    "        min_samples_leaf=20,\n",
    "        min_samples_split=40,\n",
    "    )),\n",
    "\n",
    "    # Regularize RF\n",
    "    (\"random_forest\", RandomForestRegressor(\n",
    "        n_estimators=400,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        max_depth=16,\n",
    "        min_samples_leaf=10,\n",
    "        min_samples_split=20,\n",
    "        max_features=0.7,\n",
    "    )),\n",
    "\n",
    "    # Stochastic GB + early stopping\n",
    "    (\"gradient_boosting\", GradientBoostingRegressor(\n",
    "        random_state=42,\n",
    "        n_estimators=2000,\n",
    "        learning_rate=0.03,\n",
    "        max_depth=2,\n",
    "        min_samples_leaf=20,\n",
    "        subsample=0.7,              # <- reduces overfit\n",
    "        validation_fraction=0.15,   # for early stopping\n",
    "        n_iter_no_change=30,\n",
    "        tol=1e-4,\n",
    "    )),\n",
    "]\n",
    "\n",
    "# Ridge meta-learner shrinks combination weights (helps overfit)\n",
    "meta_model = Ridge(alpha=1.0, random_state=42)\n",
    "\n",
    "# CV inside stacker so meta-model learns from out-of-fold preds\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "stack_model = StackingRegressor(\n",
    "    estimators=base_models,\n",
    "    final_estimator=meta_model,\n",
    "    passthrough=True,\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "print(\"\\n Training Ensemble Model (regularized)...\")\n",
    "stack_model.fit(X_train, y_train)\n",
    "\n",
    "# Base predictions\n",
    "stack_pred_test = stack_model.predict(X_test)\n",
    "stack_pred_train = stack_model.predict(X_train)\n",
    "\n",
    "print(\"\\n Ensemble Model Performance (Stacking Regressor):\")\n",
    "print(f\"MAE:  {mean_absolute_error(y_test, stack_pred_test):.4f}\")\n",
    "print(f\"RMSE: {sqrt(mean_squared_error(y_test, stack_pred_test)):.4f}\")\n",
    "print(f\"R¬≤:   {r2_score(y_test, stack_pred_test):.4f}\")\n",
    "prediction_match_report(y_test, stack_pred_test, \"Base Ensemble Match Rates (Test)\")\n",
    "\n",
    "# ==========================================================\n",
    "# REGULARIZED Residual Calibration (reduce overfitting)\n",
    "# ==========================================================\n",
    "train_residuals = y_train - stack_pred_train\n",
    "\n",
    "residual_model = GradientBoostingRegressor(\n",
    "    random_state=42,\n",
    "    n_estimators=2000,\n",
    "    learning_rate=0.03,\n",
    "    max_depth=1,                 # <- shallower = less overfit\n",
    "    min_samples_leaf=40,         # <- higher = smoother residuals\n",
    "    subsample=0.7,               # <- big overfit reducer\n",
    "    validation_fraction=0.15,\n",
    "    n_iter_no_change=30,\n",
    "    tol=1e-4,\n",
    ")\n",
    "\n",
    "residual_model.fit(X_train, train_residuals)\n",
    "print(\"\\n Residual correction model trained (regularized)!\")\n",
    "\n",
    "# Calibrated predictions\n",
    "test_final_pred = stack_pred_test + residual_model.predict(X_test)\n",
    "train_final_pred = stack_pred_train + residual_model.predict(X_train)\n",
    "\n",
    "print(\"\\n  Calibrated Ensemble Performance (with Residual Model):\")\n",
    "print(f\"MAE:  {mean_absolute_error(y_test, test_final_pred):.4f}\")\n",
    "print(f\"RMSE: {sqrt(mean_squared_error(y_test, test_final_pred)):.4f}\")\n",
    "print(f\"R¬≤:   {r2_score(y_test, test_final_pred):.4f}\")\n",
    "\n",
    "prediction_match_report(y_test, test_final_pred, \"Calibrated Ensemble Match Rates (Test ‚Äì Raw)\")\n",
    "prediction_match_report(y_test, np.round(test_final_pred, 2), \"Calibrated Ensemble Match Rates (Test ‚Äì Rounded to Cents)\")\n",
    "prediction_match_report(y_train, train_final_pred, \"Calibrated Ensemble Match Rates (Train)\")\n",
    "\n",
    "fit_diagnostics(\n",
    "    y_train_true=y_train,\n",
    "    y_train_pred=train_final_pred,\n",
    "    y_test_true=y_test,\n",
    "    y_test_pred=test_final_pred,\n",
    "    label=\"Calibrated Ensemble + Residual (Regularized)\"\n",
    ")\n"
   ],
   "id": "9255757d902e500f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded for Phase 3!\n",
      "Shape: (1000, 9)\n",
      "PRD-driven logic features added. New shape: (1000, 22)\n",
      "\n",
      "===== ‚úÖ Random Split Applied =====\n",
      "Training Samples: 750\n",
      "Testing Samples:  250\n",
      "\n",
      " Training Ensemble Model (regularized)...\n",
      "\n",
      " Ensemble Model Performance (Stacking Regressor):\n",
      "MAE:  67.0958\n",
      "RMSE: 92.5184\n",
      "R¬≤:   0.9591\n",
      "\n",
      "=====  Base Ensemble Match Rates (Test) =====\n",
      "Exact Match Rate (<= $0.01): 0.0000\n",
      "Close Match Rate (<= $1.00): 0.0200\n",
      "¬±$5 Accuracy: 0.0480\n",
      "\n",
      " Residual correction model trained (regularized)!\n",
      "\n",
      "  Calibrated Ensemble Performance (with Residual Model):\n",
      "MAE:  66.4409\n",
      "RMSE: 92.0679\n",
      "R¬≤:   0.9595\n",
      "\n",
      "=====  Calibrated Ensemble Match Rates (Test ‚Äì Raw) =====\n",
      "Exact Match Rate (<= $0.01): 0.0000\n",
      "Close Match Rate (<= $1.00): 0.0080\n",
      "¬±$5 Accuracy: 0.0680\n",
      "\n",
      "=====  Calibrated Ensemble Match Rates (Test ‚Äì Rounded to Cents) =====\n",
      "Exact Match Rate (<= $0.01): 0.0000\n",
      "Close Match Rate (<= $1.00): 0.0080\n",
      "¬±$5 Accuracy: 0.0680\n",
      "\n",
      "=====  Calibrated Ensemble Match Rates (Train) =====\n",
      "Exact Match Rate (<= $0.01): 0.0000\n",
      "Close Match Rate (<= $1.00): 0.0107\n",
      "¬±$5 Accuracy: 0.0520\n",
      "\n",
      "===== üîé Fit Diagnostics: Calibrated Ensemble + Residual (Regularized) =====\n",
      "Train -> MAE: 61.28 | RMSE: 86.41 | R¬≤: 0.9667 | ‚â§$1: 0.011 | ‚â§$5: 0.052\n",
      "Test  -> MAE: 66.44 | RMSE: 92.07 | R¬≤: 0.9595 | ‚â§$1: 0.008 | ‚â§$5: 0.068\n",
      "\n",
      "----- Generalization Gaps (Train - Test) -----\n",
      "MAE gap:  -5.16\n",
      "RMSE gap: -5.66\n",
      "R¬≤ gap:   0.0073\n",
      "‚â§$1 gap:  0.003\n",
      "‚â§$5 gap:  -0.016\n",
      "\n",
      "‚ö†Ô∏è Likely UNDERFITTING *for match-rate*: both train/test ‚â§$1 are low.\n"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
