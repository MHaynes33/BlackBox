---
title: "ACME Legacy Reimbursement - Project Summary"
format:
  typst-pdf:
    template-partials:
      - typst-show.typ
      - typst-template.typ
---

## Project Overview
ACME's legacy reimbursement engine has operated as a black box for decades. Our mandate is to reproduce its outputs and explain its embedded logic so stakeholders can compare legacy and modern systems with confidence. The effort balances statistical fidelity with business clarity, preserving quirks such as tiered adjustments and rounding artifacts that materially affect payouts.

## Team Roles & Responsibilities
| Team Member | Role | Focus |
|---|---|---|
| Ayushi | Technical Lead / ML Engineer | Feature engineering, modeling approach, reproducibility, pipeline design |
| Mike | Business Analyst | PRD/interview synthesis, business logic hypotheses, narrative alignment |
| Colyn | Documentation & Communication | Report structure, clarity of written deliverables, presentation flow |
| Matt | QA / Testing / Data Wrangler | Data integrity checks, spot-testing model outputs, edge-case validation |

## Executive Overview
We are reverse-engineering ACME's legacy reimbursement engine to match and explain its outputs. This report summarizes progress across three phases: initial discovery and behavioral hypotheses, feature engineering with baseline models, and correlation-driven insights to guide next modeling steps. The aim is to balance technical rigor with business clarity while preserving legacy artifacts (tiering, rounding quirks) that stakeholders care about.

## Dataset Reviewed
The dataset includes 6,000 records (1,000 public with reimbursement labels and 5,000 unlabeled private cases).
We examined the historical reimbursement dataset, which includes:

- **Trip Duration (days)**
- **Total Mileage**
- **Total Receipts**
- **Reimbursement Amount** (system output)

A data cleaning notebook was created to:
- Remove formatting inconsistencies
- Validate numeric ranges
- Prepare features for further modeling

_Result:_ A clean, analysis-ready dataset.

## Key Observed Patterns

| Factor | Observation | Interpretation |
|---|---|---|
| **Trip Length** | Reimbursement is **more generous** around **4â€“6 days**, declines for >7 days trips | Suggests a **sweet spot and long-trip penalty** |
| **Mileage** | Value-per-mile decreases after ~100 miles/day | Indicates a **non-linear mileage adjustment curve** |
| **Receipts** | Higher receipts do **not** consistently produce higher reimbursement | Suggests **diminishing returns and upper/lower spend penalties** |
| **Non-linear Behavior** | Adjustments change together, not independently | Legacy system likely uses **multiple interacting rules** |
| **Rounding Artifacts** | Some reimbursements show small irregularities | Suggests **bugs/features that must be preserved** |

## Phase 1 - Discovery, Data Quality, and Business Logic Hypotheses

### Goal
Understand the structure and quality of the public/private reimbursement data and form testable hypotheses about the legacy system's business rules.

### What we did
- Flattened public and private JSON cases into tabular data.
- Checked for missing values, duplicates, and non-positive entries across duration, miles, receipts, and reimbursement; none required removal.
- Ran IQR-based outlier scans and confirmed all points remained within reasonable business bounds.
- Produced distributions and correlation views for duration, miles, receipts, and reimbursement.
- Compared public vs private datasets to check for domain drift.

### Key evidence
- **Trip duration:** mostly 1-5 days with a small tail beyond a week; moderate correlation with reimbursement (r ~ 0.45), so duration matters but is not the main driver.
- **Miles traveled:** right-skewed; most cases under ~500 miles; strong correlation with reimbursement (r ~ 0.80).
- **Receipts:** strongest correlation with reimbursement (r ~ 0.85); high spend drives payouts but hints at diminishing returns at the top end.
- **Public vs private:** similar ranges and shapes; only minor variance differences, so combined training is reasonable.

See Figure 1 for the correlation heatmap and distribution views.

### Takeaway (business view)
The legacy engine is not purely linear. It appears to reward balanced, mid-length trips with reasonable mileage and disciplined spend, using tiered mileage adjustments and diminishing-return curves rather than straight-line rules.

```{=typst}
#pagebreak()
```
### Phase 1 visuals

The charts below summarize the main patterns:

- Correlation: receipts and miles dominate; duration contributes modestly.
- Receipts skew: long right tail underscores diminishing returns and the need for non-linear handling of high spend.
- Public vs private: overlapping distributions suggest shared logic across datasets.

```{python}
#| echo: false
#| warning: false
#| message: false
#| fig-width: 6
#| fig-height: 9
#| label: fig-phase1-key-visuals
#| fig-cap: "Phase 1 visuals: correlation structure, receipts skew, and public vs private alignment."
#| fig-align: "center"

import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

# Set base font family for plots (fallbacks to avoid missing font warnings)
plt.rcParams["font.family"] = ["DejaVu Sans", "Arial"]

# Load data (works whether execution starts at project root or within reports/)
try:
    df = pd.read_csv("data/combined_clean.csv")
except FileNotFoundError:
    df = pd.read_csv("../data/combined_clean.csv")

fig, axes = plt.subplots(3, 1, figsize=(6, 9))

corr = df[["trip_duration_days", "miles_traveled", "total_receipts_amount", "reimbursement"]].corr()
sns.heatmap(corr, annot=True, cmap="YlGnBu", fmt=".2f", ax=axes[0])
axes[0].set_title("Correlation Heatmap")

sns.histplot(df["total_receipts_amount"], kde=True, bins=30, color="seagreen", ax=axes[1])
axes[1].set_title("Receipts Distribution (skew)")
axes[1].set_xlabel("Total Receipts")

if "dataset" in df.columns:
    sns.boxplot(x="dataset", y="reimbursement", data=df, palette="Set2", ax=axes[2])
    axes[2].set_title("Public vs Private Reimbursement")
else:
    axes[2].axis("off")

plt.tight_layout()
plt.show()

```

```{=typst}
#pagebreak()
```
## Phase 2 - Feature Engineering and Baseline Modeling

### Goal
Translate Phase 1 behavioral findings into engineered efficiency features and measure how well simple baselines approximate the legacy engine.

### What we did
- Engineered rate and balance features: cost_per_day, cost_per_mile, miles_per_day, cost_ratio (cost_per_day / cost_per_mile).
- Guarded divides by zero, replaced inf/NaN, confirmed IQR checks required no row drops; used a 75/25 train/test split.
- Trained baseline models: Linear, Ridge, Lasso, and Polynomial (degree 2) on the seven engineered features.

### Key evidence
- Correlations with reimbursement: receipts (~0.70) strongest; trip duration (~0.51) and miles traveled (~0.43) moderate; rate features are weak alone but help via interactions.
- Model fit: Linear/Ridge/Lasso R^2 ~0.78 with RMSE ~200; Polynomial R^2 ~0.89 with RMSE ~142, showing non-linear terms are needed.
- Residuals for the polynomial model center near zero with a few long-tail trips driving the extremes, consistent with the skew observed in Phase 1.

### Takeaway (business view)
Linear structure covers much of the variance, but the legacy engine's tiering and diminishing-return behavior needs non-linear interactions. The engineered efficiency features add signal when combined non-linearly; regularization keeps coefficients stable.

### Phase 2 visuals

The charts below mirror the Phase 1 layout:
- Correlation heatmap across engineered features and reimbursement.
- Baseline model fit (R^2) comparison.
- Actual vs predicted for the best baseline model (polynomial degree 2).

```{python}
#| echo: false
#| warning: false
#| message: false
#| fig-width: 6
#| fig-height: 9
#| fig-align: "center"
#| label: fig-phase2-key-visuals
#| fig-cap: "Phase 2 visuals: engineered feature correlations, model fit comparison, and polynomial predictions."

import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline
from sklearn.metrics import r2_score, mean_squared_error
from math import sqrt

plt.rcParams["font.family"] = ["DejaVu Sans", "Arial"]

# Load engineered dataset (works from project root or reports/)
try:
    df = pd.read_csv("data/phase2_features_baseline_models.csv")
except FileNotFoundError:
    df = pd.read_csv("../data/phase2_features_baseline_models.csv")

features = [
    "trip_duration_days",
    "miles_traveled",
    "total_receipts_amount",
    "cost_per_day",
    "cost_per_mile",
    "miles_per_day",
    "cost_ratio",
]
target = "reimbursement"

X = df[features]
y = df[target]

split = int(0.75 * len(df))
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]

lin = LinearRegression().fit(X_train, y_train)
ridge = Ridge(alpha=1.0).fit(X_train, y_train)
lasso = Lasso(alpha=0.01).fit(X_train, y_train)
poly = make_pipeline(PolynomialFeatures(degree=2, include_bias=False), LinearRegression()).fit(X_train, y_train)

preds = {
    "Linear": lin.predict(X_test),
    "Ridge": ridge.predict(X_test),
    "Lasso": lasso.predict(X_test),
    "Poly (deg=2)": poly.predict(X_test),
}

metrics = []
for name, pred in preds.items():
    metrics.append({
        "model": name,
        "r2": r2_score(y_test, pred),
        "rmse": sqrt(mean_squared_error(y_test, pred)),
    })

metric_df = pd.DataFrame(metrics).sort_values("r2", ascending=False)

fig, axes = plt.subplots(3, 1, figsize=(6, 10))

corr = df[features + [target]].corr()
sns.heatmap(corr, annot=True, fmt=".2f", cmap="YlGnBu", ax=axes[0])
axes[0].set_title("Correlation across engineered features and reimbursement")

sns.barplot(data=metric_df, x="model", y="r2", palette="crest", ax=axes[1])
axes[1].set_ylim(0, 1)
axes[1].set_ylabel("R^2")
axes[1].set_title("Model fit (R^2)")

sns.scatterplot(x=y_test, y=preds["Poly (deg=2)"], color="teal", alpha=0.7, ax=axes[2])
axes[2].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], "r--", linewidth=1)
axes[2].set_xlabel("Actual reimbursement")
axes[2].set_ylabel("Predicted reimbursement (Poly deg=2)")
axes[2].set_title("Actual vs predicted (best baseline model)")
plt.tight_layout()
plt.show()

```

---

```{=typst}
#pagebreak()
```
## Phase 3 - Modeling Outlook & Integration Plan
**Objectives**
- Move beyond polynomial baselines to tree/ensemble methods that can capture tiering, interaction, and diminishing-return effects without manual specification.
- Harden the pipeline toward production requirements (performance, determinism, minimal dependencies).

**Planned actions**
- Add random forest and gradient boosting models; tune hyperparameters with cross-validation.
- Evaluate against business thresholds (within $0.01 / within $1.00) in addition to MAE/RMSE.
- Apply interpretability tools (SHAP, partial dependence) to translate model behavior into stakeholder-ready rules.
- Implement explicit business constraints where needed (e.g., long-trip penalties, capped daily spend, monotonic mileage effects).

---

## Next Steps and Recommendations
- Broaden models: add tree/ensemble approaches (random forest, gradient boosting) alongside polynomial baselines.
- Strengthen evaluation: introduce cross-validation and report MAE/RMSE plus business thresholds (percent within +/-$0.01 and +/-$1.00).
- Encode business effects: implement explicit long-trip penalties and spend-discipline rules (piecewise mileage rates, capped daily spend, monotonic constraints).
- Interpretability: apply SHAP/partial dependence to translate model behavior into stakeholder-friendly rules; document rounding/post-processing to preserve legacy artifacts.
- Production readiness: package the pipeline into the required 3-argument script, ensure <5s runtime, remove external dependencies, and fix seeds/versioning for deterministic outputs.
